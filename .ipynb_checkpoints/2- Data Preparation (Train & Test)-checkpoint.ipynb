{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://octodex.github.com/images/privateinvestocat.jpg\" alt=\"Kit\" title=\"Cat\" width=\"350\" height=\"200\" />\n",
    "*(image from octodex github)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:18:57.978367Z",
     "start_time": "2019-03-05T18:18:57.623364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410fa20659e64d739c8c3a0b71a2c880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name= wg.Text(value='test')\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:18:58.080375Z",
     "start_time": "2019-03-05T18:18:57.992365Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "#Dataframe packages\n",
    "import zipfile\n",
    "import json\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as wg\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T15:56:25.081214Z",
     "start_time": "2019-03-02T15:56:25.077198Z"
    }
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:19:01.256462Z",
     "start_time": "2019-03-05T18:18:58.087371Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "score=[]\n",
    "magnitude=[]\n",
    "petid=[]\n",
    "\n",
    "string = name.value + \"_sentiment.zip\"\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['documentSentiment'])\n",
    "         result_tuple = tuple(json_tree.execute('$..score'))\n",
    "         result_tuple2=tuple(json_tree.execute('$..magnitude'))\n",
    "         score.append(result_tuple)\n",
    "         magnitude.append(result_tuple2)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n",
    "                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:19:28.948187Z",
     "start_time": "2019-03-05T18:19:01.273385Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "description=[]\n",
    "topicality=[]\n",
    "imageid=[]\n",
    "\n",
    "string=name.value + \"_metadata.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "# Read Zip File and Export a Dataset with the Topicality and Description for each Image\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "       #Open the Zip File\n",
    "      with z.open(filename) as f:  \n",
    "          #Read the json File\n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          #Check if the file contains the parent label Annotations and export description and topicality\n",
    "          if 'labelAnnotations' in d:\n",
    "             json_tree = objectpath.Tree(d['labelAnnotations'])\n",
    "             image_metadata1 = list(tuple(json_tree.execute('$..description')))\n",
    "             image_metadata2 =  list(tuple(json_tree.execute('$..topicality')))\n",
    "\n",
    "             #Create a list of all descriptions and topicality\n",
    "             description.append(image_metadata1)\n",
    "             topicality.append(image_metadata2)\n",
    "         \n",
    "             #Create a list with all image id name\n",
    "             imageid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "# Prepare the output by renaming all variables\n",
    "description=pd.DataFrame(description)\n",
    "topicality=pd.DataFrame(topicality)\n",
    "\n",
    "new_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\n",
    "description.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "new_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\n",
    "topicality.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "image_metadata = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_metadata['PetID'] = image_metadata['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# TOPICALITY #\n",
    "##############\n",
    "\n",
    "image_metadata['metadata_topicality_mean'] = image_metadata.iloc[:,1:10].mean(axis=1)\n",
    "image_metadata['metadata_topicality_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n",
    "\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.iloc[:,1:10].max(axis=1)\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n",
    "\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.iloc[:,1:10].min(axis=1)\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n",
    "\n",
    "\n",
    "image_metadata['metadata_topicality_0_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\n",
    "image_metadata['metadata_topicality_0_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(max)\n",
    "image_metadata['metadata_topicality_0_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n",
    "\n",
    "\n",
    "###############\n",
    "# DESCRIPTION #\n",
    "###############\n",
    "\n",
    "# Create Features from the Images\n",
    "image_metadata['L_metadata_0_cat']=image_metadata['metadata_description_0'].str.contains(\"cat\").astype(int)\n",
    "image_metadata['L_metadata_0_dog'] =image_metadata['metadata_description_0'].str.contains(\"dog\").astype(int)\n",
    "\n",
    "image_metadata['L_metadata_any_cat']=image_metadata.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\n",
    "image_metadata['L_metadata_any_dog']=image_metadata.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n",
    "\n",
    "image_metadata['L_metadata_0_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_cat'].transform('sum')\n",
    "image_metadata['L_metadata_0_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_dog'].transform('sum')\n",
    "\n",
    "image_metadata['L_metadata_any_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_cat'].transform('sum')\n",
    "image_metadata['L_metadata_any_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_dog'].transform('sum')\n",
    "\n",
    "image_metadata = image_metadata[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\n",
    "image_metadata=image_metadata.drop_duplicates('PetID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:19:33.539329Z",
     "start_time": "2019-03-05T18:19:28.952188Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = None  \n",
    "data = None  \n",
    "\n",
    "color_score_mean=[]\n",
    "color_score_min=[]\n",
    "color_score_max=[]\n",
    "\n",
    "color_pixelfrac_mean=[]\n",
    "color_pixelfrac_min=[]\n",
    "color_pixelfrac_max=[]\n",
    "\n",
    "crops=[]\n",
    "\n",
    "imageid=[]\n",
    "\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          file_keys = list(d.keys())\n",
    "          if  'imagePropertiesAnnotation' in file_keys:\n",
    "              file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "                        \n",
    "              file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "              file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "              \n",
    "              file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n",
    "              file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n",
    "\n",
    "              \n",
    "              file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n",
    "              file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n",
    "              \n",
    "              \n",
    "          #Create a list with all image id name\n",
    "          imageid.append(filename.replace('.json',''))\n",
    "          \n",
    "          color_score_mean.append(file_color_score_mean)\n",
    "          color_score_min.append(file_color_score_min)\n",
    "          color_score_max.append(file_color_score_max)\n",
    "          \n",
    "          \n",
    "          color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n",
    "          color_pixelfrac_min.append(file_color_pixelfrac_min)\n",
    "          color_pixelfrac_max.append(file_color_pixelfrac_max)\n",
    "\n",
    "        \n",
    "image_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# COLOR INFO #\n",
    "##############\n",
    "image_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \n",
    "image_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \n",
    "image_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n",
    "\n",
    "image_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \n",
    "image_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \n",
    "image_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max) \n",
    "\n",
    "image_properties=image_properties.drop_duplicates('PetID')\n",
    "image_properties = image_properties.drop(['ImageId'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:00.573125Z",
     "start_time": "2019-03-05T18:19:33.542326Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_of_laplacian(data):\n",
    "    image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() \n",
    "\n",
    "blur=[]\n",
    "image_pixel=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "              \n",
    "              #Blur \n",
    "              data = zfile.read(filename)\n",
    "              # Pixels\n",
    "              with Image.open( BytesIO(data)) as pixel:\n",
    "                  width, height = pixel.size\n",
    "              \n",
    "              pixel = width*height\n",
    "              \n",
    "              #image pixel size for each image\n",
    "              image_pixel.append(pixel)\n",
    "              #blur for each image\n",
    "              blur.append(variance_of_laplacian(data))\n",
    "              #image id\n",
    "              imageid.append(filename.replace('.jpg',''))\n",
    "          \n",
    "          \n",
    "# Join Pixel, Blur and Image ID\n",
    "image_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n",
    "                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n",
    "\n",
    "#Mean of the Mean\n",
    "image_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\n",
    "image_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n",
    "\n",
    "image_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \n",
    "image_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n",
    "\n",
    "image_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \n",
    "image_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n",
    "\n",
    "image_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\n",
    "image_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n",
    "\n",
    "\n",
    "image_quality = image_quality.drop(['blur','pixel','ImageId'], 1)\n",
    "image_quality=image_quality.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:03.344818Z",
     "start_time": "2019-03-05T18:21:00.575968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "if name.value == \"train\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/train.csv')\n",
    "    \n",
    "    \n",
    "if name.value == \"test\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "\n",
    "#test = pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "breed =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\n",
    "color =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/color_labels.csv') #A pet could have multiple colors\n",
    "state =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/state_labels.csv')\n",
    "\n",
    "# Add information about color, breed, state and sentiment data\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, state,  how='left', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_metadata,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "\n",
    "# Add information about quality Images\n",
    "df= (pd.merge(df, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:03.421822Z",
     "start_time": "2019-03-05T18:21:03.346818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>Description</th>\n",
       "      <th>PetID</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>BreedName1</th>\n",
       "      <th>BreedName2</th>\n",
       "      <th>ColorName1</th>\n",
       "      <th>ColorName2</th>\n",
       "      <th>ColorName3</th>\n",
       "      <th>StateName</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>metadata_topicality_max</th>\n",
       "      <th>metadata_topicality_mean</th>\n",
       "      <th>metadata_topicality_min</th>\n",
       "      <th>metadata_topicality_0_mean</th>\n",
       "      <th>metadata_topicality_0_max</th>\n",
       "      <th>metadata_topicality_0_min</th>\n",
       "      <th>L_metadata_0_cat_sum</th>\n",
       "      <th>L_metadata_0_dog_sum</th>\n",
       "      <th>L_metadata_any_cat_sum</th>\n",
       "      <th>L_metadata_any_dog_sum</th>\n",
       "      <th>metadata_color_pixelfrac_mean</th>\n",
       "      <th>metadata_color_pixelfrac_min</th>\n",
       "      <th>metadata_color_pixelfrac_max</th>\n",
       "      <th>metadata_color_score_mean</th>\n",
       "      <th>metadata_color_score_min</th>\n",
       "      <th>metadata_color_score_max</th>\n",
       "      <th>pixel_mean</th>\n",
       "      <th>blur_mean</th>\n",
       "      <th>pixel_min</th>\n",
       "      <th>blur_min</th>\n",
       "      <th>pixel_max</th>\n",
       "      <th>blur_max</th>\n",
       "      <th>pixel_sum</th>\n",
       "      <th>blur_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Puppy</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41326</td>\n",
       "      <td>4475f31553f0170229455e3c5645644f</td>\n",
       "      <td>0</td>\n",
       "      <td>Puppy is calm for a young dog, but he becomes ...</td>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.965361</td>\n",
       "      <td>0.791512</td>\n",
       "      <td>0.521410</td>\n",
       "      <td>0.956653</td>\n",
       "      <td>0.965361</td>\n",
       "      <td>0.940461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.187738</td>\n",
       "      <td>0.089559</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.432879</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>1301.425223</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>801.752814</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>1902.830546</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3904.275670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>London</td>\n",
       "      <td>24</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>4475f31553f0170229455e3c5645644f</td>\n",
       "      <td>0</td>\n",
       "      <td>Urgently seeking adoption. Please contact for ...</td>\n",
       "      <td>73c10e136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Domestic Short Hair</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992336</td>\n",
       "      <td>0.798036</td>\n",
       "      <td>0.615891</td>\n",
       "      <td>0.992336</td>\n",
       "      <td>0.992336</td>\n",
       "      <td>0.992336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088742</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.266022</td>\n",
       "      <td>0.099007</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.419909</td>\n",
       "      <td>96400.0</td>\n",
       "      <td>1437.203419</td>\n",
       "      <td>96400.0</td>\n",
       "      <td>1437.203419</td>\n",
       "      <td>96400.0</td>\n",
       "      <td>1437.203419</td>\n",
       "      <td>96400.0</td>\n",
       "      <td>1437.203419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Snowball</td>\n",
       "      <td>20</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41326</td>\n",
       "      <td>4475f31553f0170229455e3c5645644f</td>\n",
       "      <td>0</td>\n",
       "      <td>Snowball... doesn't look so good (she is healt...</td>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Domestic Short Hair</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>0.811257</td>\n",
       "      <td>0.561695</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064993</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.189213</td>\n",
       "      <td>0.096340</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.404553</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2545.769339</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2545.769339</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2545.769339</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2545.769339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type      Name  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
       "0     1     Puppy    2     307       0       1       1       0       0   \n",
       "1     2    London   24     266       0       1       2       7       0   \n",
       "2     2  Snowball   20     266       0       2       7       0       0   \n",
       "\n",
       "   MaturitySize  FurLength  Vaccinated  Dewormed  Sterilized  Health  \\\n",
       "0             2          2           2         2           2       1   \n",
       "1             2          1           1         1           1       1   \n",
       "2             2          1           1         1           1       1   \n",
       "\n",
       "   Quantity  Fee  State                         RescuerID  VideoAmt  \\\n",
       "0         1  150  41326  4475f31553f0170229455e3c5645644f         0   \n",
       "1         1    0  41326  4475f31553f0170229455e3c5645644f         0   \n",
       "2         1  150  41326  4475f31553f0170229455e3c5645644f         0   \n",
       "\n",
       "                                         Description      PetID  PhotoAmt  \\\n",
       "0  Puppy is calm for a young dog, but he becomes ...  378fcc4fc       3.0   \n",
       "1  Urgently seeking adoption. Please contact for ...  73c10e136       1.0   \n",
       "2  Snowball... doesn't look so good (she is healt...  72000c4c5       1.0   \n",
       "\n",
       "            BreedName1 BreedName2 ColorName1 ColorName2 ColorName3 StateName  \\\n",
       "0          Mixed Breed        NaN      Black        NaN        NaN  Selangor   \n",
       "1  Domestic Short Hair        NaN      Brown      White        NaN  Selangor   \n",
       "2  Domestic Short Hair        NaN      White        NaN        NaN  Selangor   \n",
       "\n",
       "   sentiment_document_score  sentiment_document_magnitude  \\\n",
       "0                       0.7                           1.5   \n",
       "1                       0.0                           0.0   \n",
       "2                       0.1                           1.5   \n",
       "\n",
       "   metadata_topicality_max  metadata_topicality_mean  metadata_topicality_min  \\\n",
       "0                 0.965361                  0.791512                 0.521410   \n",
       "1                 0.992336                  0.798036                 0.615891   \n",
       "2                 0.988691                  0.811257                 0.561695   \n",
       "\n",
       "   metadata_topicality_0_mean  metadata_topicality_0_max  \\\n",
       "0                    0.956653                   0.965361   \n",
       "1                    0.992336                   0.992336   \n",
       "2                    0.988691                   0.988691   \n",
       "\n",
       "   metadata_topicality_0_min  L_metadata_0_cat_sum  L_metadata_0_dog_sum  \\\n",
       "0                   0.940461                   0.0                   3.0   \n",
       "1                   0.992336                   1.0                   0.0   \n",
       "2                   0.988691                   1.0                   0.0   \n",
       "\n",
       "   L_metadata_any_cat_sum  L_metadata_any_dog_sum  \\\n",
       "0                     0.0                     3.0   \n",
       "1                     1.0                     0.0   \n",
       "2                     1.0                     0.0   \n",
       "\n",
       "   metadata_color_pixelfrac_mean  metadata_color_pixelfrac_min  \\\n",
       "0                       0.073456                      0.003729   \n",
       "1                       0.088742                      0.001183   \n",
       "2                       0.064993                      0.001648   \n",
       "\n",
       "   metadata_color_pixelfrac_max  metadata_color_score_mean  \\\n",
       "0                      0.187738                   0.089559   \n",
       "1                      0.266022                   0.099007   \n",
       "2                      0.189213                   0.096340   \n",
       "\n",
       "   metadata_color_score_min  metadata_color_score_max  pixel_mean  \\\n",
       "0                  0.010678                  0.432879    120000.0   \n",
       "1                  0.002527                  0.419909     96400.0   \n",
       "2                  0.011427                  0.404553    120000.0   \n",
       "\n",
       "     blur_mean  pixel_min     blur_min  pixel_max     blur_max  pixel_sum  \\\n",
       "0  1301.425223   120000.0   801.752814   120000.0  1902.830546   360000.0   \n",
       "1  1437.203419    96400.0  1437.203419    96400.0  1437.203419    96400.0   \n",
       "2  2545.769339   120000.0  2545.769339   120000.0  2545.769339   120000.0   \n",
       "\n",
       "      blur_sum  \n",
       "0  3904.275670  \n",
       "1  1437.203419  \n",
       "2  2545.769339  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malaysia Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:03.561989Z",
     "start_time": "2019-03-05T18:21:03.424822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area ={\n",
    "    41336:19102,\n",
    "41325:9500,\n",
    "41367:15099,\n",
    "41401:243,\n",
    "41415:91,\n",
    "41324:1664,\n",
    "41332:6686,\n",
    "41335:36137,\n",
    "41330:21035,\n",
    "41380:821,\n",
    "41327:1048,\n",
    "41345:73631,\n",
    "41342:124450,\n",
    "41326:8104,\n",
    "41361:13035}\n",
    "\n",
    "\n",
    "df[\"state_gdp\"] = df.State.map(state_gdp)\n",
    "df[\"state_population\"] = df.State.map(state_population)\n",
    "df[\"state_area\"] = df.State.map(state_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:03.856994Z",
     "start_time": "2019-03-05T18:21:03.563989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n",
    "df['L_Color1'] = (pd.isnull(df['ColorName3']) & pd.isnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color2'] = (pd.isnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color3'] = (pd.notnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "\n",
    "# Breed (create a flag if the pet has 1 breed or 2)\n",
    "df['L_Breed1'] = (pd.isnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "df['L_Breed2'] = (pd.notnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "\n",
    "# Breed create columns\n",
    "df['L_Breed1_Siamese'] =(df['BreedName1']=='Siamese').astype(int)\n",
    "df['L_Breed1_Persian']=(df['BreedName1']=='Persian').astype(int)\n",
    "df['L_Breed1_Labrador_Retriever']=(df['BreedName1']=='Labrador Retriever').astype(int)\n",
    "df['L_Breed1_Terrier']=(df['BreedName1']=='Terrier').astype(int)\n",
    "df['L_Breed1_Golden_Retriever ']=(df['BreedName1']=='Golden Retriever').astype(int)\n",
    "\n",
    "#Name (create a flag if the name is missing, with less than two letters)\n",
    "df['L_Name_missing'] =  (pd.isnull(df['Name'])).astype(int)\n",
    "df['Name_Length']=df['Name'].str.len() \n",
    "\n",
    "#Description \n",
    "df['Description_Length']=df['Description'].str.len() \n",
    "\n",
    "# Fee Amount\n",
    "df['L_Fee_Free'] =  (df['Fee']==0).astype(int)\n",
    "\n",
    "#Add the Number of Pets per Rescuer \n",
    "pets_total = df.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n",
    "df= pd.merge(df, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n",
    "df.count()\n",
    "\n",
    "# No photo\n",
    "df['L_NoPhoto'] =  (df['PhotoAmt']==0).astype(int)\n",
    "\n",
    "#No Video\n",
    "df['L_NoVideo'] =  (df['VideoAmt']==0).astype(int)\n",
    "\n",
    "#Log Age \n",
    "df['Log_Age']= np.log(df.Age+1) \n",
    "\n",
    "#Negative Score \n",
    "df['L_scoreneg'] =  (df['sentiment_document_score']<0).astype(int)\n",
    "\n",
    "#Quantity Amount >5\n",
    "df.loc[df['Quantity'] > 5, 'Quantity'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:09.416965Z",
     "start_time": "2019-03-05T18:21:03.858992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the Variable Description\n",
    "df['Description'] =df['Description'].fillna(\"<MISSING>\")\n",
    "df['Description'] = df['Description'].str.replace('\\d+', '')\n",
    "df['Description'] = df['Description'].str.lower()\n",
    "df[\"Description\"] = df['Description'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Stop Words \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "df['Description'] = df['Description'].str.replace(pat, '')\n",
    "df['Description'] = df['Description'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Stem Words\n",
    "df['Description'] = df['Description'].astype(str).str.split()\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['Description']=df['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n",
    "\n",
    "df['Description']=df['Description'].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:10.144961Z",
     "start_time": "2019-03-05T18:21:09.419964Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "# Matrix Factorization for dimensionality reduction\n",
    "\n",
    "svd_ = TruncatedSVD(\n",
    "    n_components=5, random_state=1337)\n",
    "nmf_ = NMF(\n",
    "    n_components=5, random_state=1337)\n",
    "\n",
    "tfidf_col = TfidfVectorizer().fit_transform(df['Description'])\n",
    "svd_col = svd_.fit_transform(tfidf_col)\n",
    "svd_col = pd.DataFrame(svd_col)\n",
    "svd_col = svd_col.add_prefix('SVD_')\n",
    "\n",
    "nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "nmf_col = pd.DataFrame(nmf_col)\n",
    "nmf_col = nmf_col.add_prefix('NMF_')\n",
    "df = pd.concat([df,nmf_col,svd_col],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:10.177967Z",
     "start_time": "2019-03-05T18:21:10.152969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cannot be used for this analysis (IDs, Texts...)\n",
    "df = df.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n",
    "              'BreedName1','Color1', 'Color2', 'Color3','Age','State'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:10.242967Z",
     "start_time": "2019-03-05T18:21:10.185971Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n",
    "           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n",
    "           'blur_min','blur_max','blur_sum','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n",
    "           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n",
    "            'Name_Length','Description_Length']:\n",
    "    \n",
    "\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "    \n",
    "# replacing na values with No Color \n",
    "df[\"ColorName2\"].fillna(\"No Color\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:10.588992Z",
     "start_time": "2019-03-05T18:21:10.251971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "df = pd.concat([df.drop('StateName', axis=1),pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "\n",
    "col=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\n",
    "for i in col:\n",
    "    df = pd.concat([df.drop(i, axis=1),pd.get_dummies(df[i], prefix=i)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:31:42.297514Z",
     "start_time": "2019-03-05T18:31:41.626423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_features.csv\n"
     ]
    }
   ],
   "source": [
    "output= name.value + \"_features.csv\"\n",
    "print(output)\n",
    "filepath= os.path.join(\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\", output)\n",
    "df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "200px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.233334,
   "position": {
    "height": "40px",
    "left": "1566px",
    "right": "4px",
    "top": "140px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
