{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://octodex.github.com/images/privateinvestocat.jpg\" alt=\"Kit\" title=\"Cat\" width=\"350\" height=\"200\" />\n",
    "*(image from octodex github)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:47:51.782771Z",
     "start_time": "2019-03-04T17:47:51.527811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dda046f35040f69ba644edc07d3092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name= wg.Text(value='train')\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:47:51.843790Z",
     "start_time": "2019-03-04T17:47:51.791767Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "#Dataframe packages\n",
    "import zipfile\n",
    "import json\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as wg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T15:56:25.081214Z",
     "start_time": "2019-03-02T15:56:25.077198Z"
    }
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:47:58.131421Z",
     "start_time": "2019-03-04T17:47:51.849792Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "score=[]\n",
    "magnitude=[]\n",
    "petid=[]\n",
    "\n",
    "string = name.value + \"_sentiment.zip\"\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['documentSentiment'])\n",
    "         result_tuple = tuple(json_tree.execute('$..score'))\n",
    "         result_tuple2=tuple(json_tree.execute('$..magnitude'))\n",
    "         score.append(result_tuple)\n",
    "         magnitude.append(result_tuple2)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n",
    "                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:49:27.664195Z",
     "start_time": "2019-03-04T17:47:58.141423Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "description=[]\n",
    "topicality=[]\n",
    "imageid=[]\n",
    "\n",
    "string=name.value + \"_metadata.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "# Read Zip File and Export a Dataset with the Topicality and Description for each Image\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "       #Open the Zip File\n",
    "      with z.open(filename) as f:  \n",
    "          #Read the json File\n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          #Check if the file contains the parent label Annotations and export description and topicality\n",
    "          if 'labelAnnotations' in d:\n",
    "             json_tree = objectpath.Tree(d['labelAnnotations'])\n",
    "             image_metadata1 = list(tuple(json_tree.execute('$..description')))\n",
    "             image_metadata2 =  list(tuple(json_tree.execute('$..topicality')))\n",
    "\n",
    "             #Create a list of all descriptions and topicality\n",
    "             description.append(image_metadata1)\n",
    "             topicality.append(image_metadata2)\n",
    "         \n",
    "             #Create a list with all image id name\n",
    "             imageid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "# Prepare the output by renaming all variables\n",
    "description=pd.DataFrame(description)\n",
    "topicality=pd.DataFrame(topicality)\n",
    "\n",
    "new_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\n",
    "description.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "new_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\n",
    "topicality.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "image_metadata = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_metadata['PetID'] = image_metadata['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# TOPICALITY #\n",
    "##############\n",
    "\n",
    "image_metadata['metadata_topicality_mean'] = image_metadata.iloc[:,1:10].mean(axis=1)\n",
    "image_metadata['metadata_topicality_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n",
    "\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.iloc[:,1:10].max(axis=1)\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n",
    "\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.iloc[:,1:10].min(axis=1)\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n",
    "\n",
    "\n",
    "image_metadata['metadata_topicality_0_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\n",
    "image_metadata['metadata_topicality_0_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(max)\n",
    "image_metadata['metadata_topicality_0_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n",
    "\n",
    "\n",
    "###############\n",
    "# DESCRIPTION #\n",
    "###############\n",
    "\n",
    "# Create Features from the Images\n",
    "image_metadata['L_metadata_0_cat']=image_metadata['metadata_description_0'].str.contains(\"cat\").astype(int)\n",
    "image_metadata['L_metadata_0_dog'] =image_metadata['metadata_description_0'].str.contains(\"dog\").astype(int)\n",
    "\n",
    "image_metadata['L_metadata_any_cat']=image_metadata.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\n",
    "image_metadata['L_metadata_any_dog']=image_metadata.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n",
    "\n",
    "image_metadata['L_metadata_0_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_cat'].transform('sum')\n",
    "image_metadata['L_metadata_0_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_dog'].transform('sum')\n",
    "\n",
    "image_metadata['L_metadata_any_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_cat'].transform('sum')\n",
    "image_metadata['L_metadata_any_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_dog'].transform('sum')\n",
    "\n",
    "image_metadata = image_metadata[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\n",
    "image_metadata=image_metadata.drop_duplicates('PetID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:49:44.935193Z",
     "start_time": "2019-03-04T17:49:27.668201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = None  \n",
    "data = None  \n",
    "\n",
    "color_score_mean=[]\n",
    "color_score_min=[]\n",
    "color_score_max=[]\n",
    "\n",
    "color_pixelfrac_mean=[]\n",
    "color_pixelfrac_min=[]\n",
    "color_pixelfrac_max=[]\n",
    "\n",
    "crops=[]\n",
    "\n",
    "imageid=[]\n",
    "\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          file_keys = list(d.keys())\n",
    "          if  'imagePropertiesAnnotation' in file_keys:\n",
    "              file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "                        \n",
    "              file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "              file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "              \n",
    "              file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n",
    "              file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n",
    "\n",
    "              \n",
    "              file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n",
    "              file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n",
    "              \n",
    "              \n",
    "          #Create a list with all image id name\n",
    "          imageid.append(filename.replace('.json',''))\n",
    "          \n",
    "          color_score_mean.append(file_color_score_mean)\n",
    "          color_score_min.append(file_color_score_min)\n",
    "          color_score_max.append(file_color_score_max)\n",
    "          \n",
    "          \n",
    "          color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n",
    "          color_pixelfrac_min.append(file_color_pixelfrac_min)\n",
    "          color_pixelfrac_max.append(file_color_pixelfrac_max)\n",
    "\n",
    "        \n",
    "image_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# COLOR INFO #\n",
    "##############\n",
    "image_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \n",
    "image_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \n",
    "image_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n",
    "\n",
    "image_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \n",
    "image_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \n",
    "image_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max) \n",
    "\n",
    "image_properties=image_properties.drop_duplicates('PetID')\n",
    "image_properties = image_properties.drop(['ImageId'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:32.198726Z",
     "start_time": "2019-03-04T17:49:44.937197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_of_laplacian(data):\n",
    "    image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() \n",
    "\n",
    "blur=[]\n",
    "image_pixel=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "              \n",
    "              #Blur \n",
    "              data = zfile.read(filename)\n",
    "              # Pixels\n",
    "              with Image.open( BytesIO(data)) as pixel:\n",
    "                  width, height = pixel.size\n",
    "              \n",
    "              pixel = width*height\n",
    "              \n",
    "              #image pixel size for each image\n",
    "              image_pixel.append(pixel)\n",
    "              #blur for each image\n",
    "              blur.append(variance_of_laplacian(data))\n",
    "              #image id\n",
    "              imageid.append(filename.replace('.jpg',''))\n",
    "          \n",
    "          \n",
    "# Join Pixel, Blur and Image ID\n",
    "image_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n",
    "                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n",
    "\n",
    "#Mean of the Mean\n",
    "image_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\n",
    "image_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n",
    "\n",
    "image_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \n",
    "image_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n",
    "\n",
    "image_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \n",
    "image_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n",
    "\n",
    "image_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\n",
    "image_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n",
    "\n",
    "\n",
    "image_quality = image_quality.drop(['blur','pixel','ImageId'], 1)\n",
    "image_quality=image_quality.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:38.667437Z",
     "start_time": "2019-03-04T17:56:32.201727Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "if name.value == \"train\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/train.csv')\n",
    "    \n",
    "    \n",
    "if name.value == \"test\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "\n",
    "#test = pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "breed =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\n",
    "color =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/color_labels.csv') #A pet could have multiple colors\n",
    "state =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/state_labels.csv')\n",
    "\n",
    "# Add information about color, breed, state and sentiment data\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, state,  how='left', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_metadata,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "\n",
    "# Add information about quality Images\n",
    "df= (pd.merge(df, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:38.744486Z",
     "start_time": "2019-03-04T17:56:38.669437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>Description</th>\n",
       "      <th>PetID</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>BreedName1</th>\n",
       "      <th>BreedName2</th>\n",
       "      <th>ColorName1</th>\n",
       "      <th>ColorName2</th>\n",
       "      <th>ColorName3</th>\n",
       "      <th>StateName</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>metadata_topicality_max</th>\n",
       "      <th>metadata_topicality_mean</th>\n",
       "      <th>metadata_topicality_min</th>\n",
       "      <th>metadata_topicality_0_mean</th>\n",
       "      <th>metadata_topicality_0_max</th>\n",
       "      <th>metadata_topicality_0_min</th>\n",
       "      <th>L_metadata_0_cat_sum</th>\n",
       "      <th>L_metadata_0_dog_sum</th>\n",
       "      <th>L_metadata_any_cat_sum</th>\n",
       "      <th>L_metadata_any_dog_sum</th>\n",
       "      <th>metadata_color_pixelfrac_mean</th>\n",
       "      <th>metadata_color_pixelfrac_min</th>\n",
       "      <th>metadata_color_pixelfrac_max</th>\n",
       "      <th>metadata_color_score_mean</th>\n",
       "      <th>metadata_color_score_min</th>\n",
       "      <th>metadata_color_score_max</th>\n",
       "      <th>pixel_mean</th>\n",
       "      <th>blur_mean</th>\n",
       "      <th>pixel_min</th>\n",
       "      <th>blur_min</th>\n",
       "      <th>pixel_max</th>\n",
       "      <th>blur_max</th>\n",
       "      <th>pixel_sum</th>\n",
       "      <th>blur_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Nibble</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41326</td>\n",
       "      <td>8480853f516546f6cf33aa88cd76c379</td>\n",
       "      <td>0</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>86e1089a3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>0.830798</td>\n",
       "      <td>0.680083</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.393910</td>\n",
       "      <td>0.074838</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>172800.0</td>\n",
       "      <td>672.771047</td>\n",
       "      <td>172800.0</td>\n",
       "      <td>672.771047</td>\n",
       "      <td>172800.0</td>\n",
       "      <td>672.771047</td>\n",
       "      <td>172800.0</td>\n",
       "      <td>672.771047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>No Name Yet</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41401</td>\n",
       "      <td>3082c7125d8fb66f7dd4bff4192c8b14</td>\n",
       "      <td>0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>6296e909a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.985912</td>\n",
       "      <td>0.805835</td>\n",
       "      <td>0.605804</td>\n",
       "      <td>0.983590</td>\n",
       "      <td>0.985912</td>\n",
       "      <td>0.981269</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.063681</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.261856</td>\n",
       "      <td>0.091708</td>\n",
       "      <td>0.007781</td>\n",
       "      <td>0.358486</td>\n",
       "      <td>135211.5</td>\n",
       "      <td>332.307966</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>210.609272</td>\n",
       "      <td>150423.0</td>\n",
       "      <td>454.006659</td>\n",
       "      <td>270423.0</td>\n",
       "      <td>664.615931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Brisco</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>fa90fa5b1ee11c86938398b60abc32cb</td>\n",
       "      <td>0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>3422e4906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.973261</td>\n",
       "      <td>0.802988</td>\n",
       "      <td>0.500759</td>\n",
       "      <td>0.965689</td>\n",
       "      <td>0.973261</td>\n",
       "      <td>0.955734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.462192</td>\n",
       "      <td>0.094292</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.431694</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>398.978193</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>60.519822</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>1817.619512</td>\n",
       "      <td>840000.0</td>\n",
       "      <td>2792.847353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type         Name  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
       "0     2       Nibble    3     299       0       1       1       7       0   \n",
       "1     2  No Name Yet    1     265       0       1       1       2       0   \n",
       "2     1       Brisco    1     307       0       1       2       7       0   \n",
       "\n",
       "   MaturitySize  FurLength  Vaccinated  Dewormed  Sterilized  Health  \\\n",
       "0             1          1           2         2           2       1   \n",
       "1             2          2           3         3           3       1   \n",
       "2             2          2           1         1           2       1   \n",
       "\n",
       "   Quantity  Fee  State                         RescuerID  VideoAmt  \\\n",
       "0         1  100  41326  8480853f516546f6cf33aa88cd76c379         0   \n",
       "1         1    0  41401  3082c7125d8fb66f7dd4bff4192c8b14         0   \n",
       "2         1    0  41326  fa90fa5b1ee11c86938398b60abc32cb         0   \n",
       "\n",
       "                                         Description      PetID  PhotoAmt  \\\n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...  86e1089a3       1.0   \n",
       "1  I just found it alone yesterday near my apartm...  6296e909a       2.0   \n",
       "2  Their pregnant mother was dumped by her irresp...  3422e4906       7.0   \n",
       "\n",
       "   AdoptionSpeed            BreedName1 BreedName2 ColorName1 ColorName2  \\\n",
       "0              2                 Tabby        NaN      Black      White   \n",
       "1              0  Domestic Medium Hair        NaN      Black      Brown   \n",
       "2              3           Mixed Breed        NaN      Brown      White   \n",
       "\n",
       "  ColorName3     StateName  sentiment_document_score  \\\n",
       "0        NaN      Selangor                       0.3   \n",
       "1        NaN  Kuala Lumpur                      -0.2   \n",
       "2        NaN      Selangor                       0.2   \n",
       "\n",
       "   sentiment_document_magnitude  metadata_topicality_max  \\\n",
       "0                           2.4                 0.990786   \n",
       "1                           0.7                 0.985912   \n",
       "2                           3.7                 0.973261   \n",
       "\n",
       "   metadata_topicality_mean  metadata_topicality_min  \\\n",
       "0                  0.830798                 0.680083   \n",
       "1                  0.805835                 0.605804   \n",
       "2                  0.802988                 0.500759   \n",
       "\n",
       "   metadata_topicality_0_mean  metadata_topicality_0_max  \\\n",
       "0                    0.990786                   0.990786   \n",
       "1                    0.983590                   0.985912   \n",
       "2                    0.965689                   0.973261   \n",
       "\n",
       "   metadata_topicality_0_min  L_metadata_0_cat_sum  L_metadata_0_dog_sum  \\\n",
       "0                   0.990786                   1.0                   0.0   \n",
       "1                   0.981269                   2.0                   0.0   \n",
       "2                   0.955734                   0.0                   7.0   \n",
       "\n",
       "   L_metadata_any_cat_sum  L_metadata_any_dog_sum  \\\n",
       "0                     1.0                     0.0   \n",
       "1                     2.0                     2.0   \n",
       "2                     0.0                     7.0   \n",
       "\n",
       "   metadata_color_pixelfrac_mean  metadata_color_pixelfrac_min  \\\n",
       "0                       0.066331                      0.000080   \n",
       "1                       0.063681                      0.005463   \n",
       "2                       0.073497                      0.000347   \n",
       "\n",
       "   metadata_color_pixelfrac_max  metadata_color_score_mean  \\\n",
       "0                      0.393910                   0.074838   \n",
       "1                      0.261856                   0.091708   \n",
       "2                      0.462192                   0.094292   \n",
       "\n",
       "   metadata_color_score_min  metadata_color_score_max  pixel_mean   blur_mean  \\\n",
       "0                  0.003673                  0.302789    172800.0  672.771047   \n",
       "1                  0.007781                  0.358486    135211.5  332.307966   \n",
       "2                  0.004190                  0.431694    120000.0  398.978193   \n",
       "\n",
       "   pixel_min    blur_min  pixel_max     blur_max  pixel_sum     blur_sum  \n",
       "0   172800.0  672.771047   172800.0   672.771047   172800.0   672.771047  \n",
       "1   120000.0  210.609272   150423.0   454.006659   270423.0   664.615931  \n",
       "2   120000.0   60.519822   120000.0  1817.619512   840000.0  2792.847353  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malaysia Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T20:16:09.425488Z",
     "start_time": "2019-03-04T20:16:08.646063Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "df[\"state_gdp\"] = df.State.map(state_gdp)\n",
    "df[\"state_population\"] = df.State.map(state_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:39.106564Z",
     "start_time": "2019-03-04T17:56:38.789508Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ColorName3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ColorName3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-9d952ef502f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L_Color1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L_Color2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L_Color3'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ColorName1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ColorName3'"
     ]
    }
   ],
   "source": [
    "# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n",
    "df['L_Color1'] = (pd.isnull(df['ColorName3']) & pd.isnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color2'] = (pd.isnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color3'] = (pd.notnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "\n",
    "# Breed (create a flag if the pet has 1 breed or 2)\n",
    "df['L_Breed1'] = (pd.isnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "df['L_Breed2'] = (pd.notnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "\n",
    "# Breed create columns\n",
    "df['L_Breed1_Siamese'] =(df['BreedName1']=='Siamese').astype(int)\n",
    "df['L_Breed1_Persian']=(df['BreedName1']=='Persian').astype(int)\n",
    "df['L_Breed1_Labrador_Retriever']=(df['BreedName1']=='Labrador Retriever').astype(int)\n",
    "df['L_Breed1_Terrier']=(df['BreedName1']=='Terrier').astype(int)\n",
    "df['L_Breed1_Golden_Retriever ']=(df['BreedName1']=='Golden Retriever').astype(int)\n",
    "\n",
    "#Name (create a flag if the name is missing, with less than two letters)\n",
    "df['L_Name_missing'] =  (pd.isnull(df['Name'])).astype(int)\n",
    "df['Name_Length']=df['Name'].str.len() \n",
    "\n",
    "#Description \n",
    "df['Description_Length']=df['Description'].str.len() \n",
    "\n",
    "# Fee Amount\n",
    "df['L_Fee_Free'] =  (df['Fee']==0).astype(int)\n",
    "\n",
    "#Add the Number of Pets per Rescuer \n",
    "pets_total = df.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n",
    "df= pd.merge(df, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n",
    "df.count()\n",
    "\n",
    "# No photo\n",
    "df['L_NoPhoto'] =  (df['PhotoAmt']==0).astype(int)\n",
    "\n",
    "#No Video\n",
    "df['L_NoVideo'] =  (df['VideoAmt']==0).astype(int)\n",
    "\n",
    "#Log Age \n",
    "df['Log_Age']= np.log(df.Age+1) \n",
    "\n",
    "#Negative Score \n",
    "df['L_scoreneg'] =  (df['sentiment_document_score']<0).astype(int)\n",
    "\n",
    "#Quantity Amount >5\n",
    "df.loc[df['Quantity'] > 5, 'Quantity'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the Variable Description\n",
    "df['Description'] =df['Description'].fillna(\"<MISSING>\")\n",
    "df['Description'] = df['Description'].str.replace('\\d+', '')\n",
    "df['Description'] = df['Description'].str.lower()\n",
    "df[\"Description\"] = df['Description'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Stop Words \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "df['Description'] = df['Description'].str.replace(pat, '')\n",
    "df['Description'] = df['Description'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Stem Words\n",
    "df['Description'] = df['Description'].astype(str).str.split()\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['Description']=df['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n",
    "\n",
    "df['Description']=df['Description'].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:39.118572Z",
     "start_time": "2019-03-04T17:47:52.054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cannot be used for this analysis (IDs, Texts...)\n",
    "df = df.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n",
    "              'BreedName1','Color1', 'Color2', 'Color3','Age','State'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:39.121571Z",
     "start_time": "2019-03-04T17:47:52.067Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n",
    "           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n",
    "           'blur_min','blur_max','blur_sum','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n",
    "           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n",
    "            'Name_Length','Description_Length']:\n",
    "    \n",
    "\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "    \n",
    "# replacing na values with No Color \n",
    "df[\"ColorName2\"].fillna(\"No Color\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:39.129570Z",
     "start_time": "2019-03-04T17:47:52.095Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "df = pd.concat([df.drop('StateName', axis=1),pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "\n",
    "col=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\n",
    "for i in col:\n",
    "    df = pd.concat([df.drop(i, axis=1),pd.get_dummies(df[i], prefix=i)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T17:56:39.132571Z",
     "start_time": "2019-03-04T17:47:52.100Z"
    }
   },
   "outputs": [],
   "source": [
    "output= name.value + \"_features.csv\"\n",
    "print(output)\n",
    "filepath= os.path.join(\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\", output)\n",
    "df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.233334,
   "position": {
    "height": "40px",
    "left": "1566px",
    "right": "4px",
    "top": "140px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
