{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://octodex.github.com/images/privateinvestocat.jpg\" alt=\"Kit\" title=\"Cat\" width=\"350\" height=\"200\" />\n",
    "*(image from octodex github)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:41:37.269538Z",
     "start_time": "2019-03-02T16:41:37.249538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e35d9d05f541debe164532aa9d2bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name= wg.Text(value='train')\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:41:37.290537Z",
     "start_time": "2019-03-02T16:41:37.273540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "#Dataframe packages\n",
    "import zipfile\n",
    "import json\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as wg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T15:56:25.081214Z",
     "start_time": "2019-03-02T15:56:25.077198Z"
    }
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:41:41.644741Z",
     "start_time": "2019-03-02T16:41:37.323541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "score=[]\n",
    "magnitude=[]\n",
    "petid=[]\n",
    "\n",
    "string = name.value + \"_sentiment.zip\"\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['documentSentiment'])\n",
    "         result_tuple = tuple(json_tree.execute('$..score'))\n",
    "         result_tuple2=tuple(json_tree.execute('$..magnitude'))\n",
    "         score.append(result_tuple)\n",
    "         magnitude.append(result_tuple2)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n",
    "                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:42:53.654551Z",
     "start_time": "2019-03-02T16:41:41.647569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "description=[]\n",
    "topicality=[]\n",
    "imageid=[]\n",
    "\n",
    "string=name.value + \"_metadata.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "# Read Zip File and Export a Dataset with the Topicality and Description for each Image\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "       #Open the Zip File\n",
    "      with z.open(filename) as f:  \n",
    "          #Read the json File\n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          #Check if the file contains the parent label Annotations and export description and topicality\n",
    "          if 'labelAnnotations' in d:\n",
    "             json_tree = objectpath.Tree(d['labelAnnotations'])\n",
    "             image_metadata1 = list(tuple(json_tree.execute('$..description')))\n",
    "             image_metadata2 =  list(tuple(json_tree.execute('$..topicality')))\n",
    "\n",
    "             #Create a list of all descriptions and topicality\n",
    "             description.append(image_metadata1)\n",
    "             topicality.append(image_metadata2)\n",
    "         \n",
    "             #Create a list with all image id name\n",
    "             imageid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "# Prepare the output by renaming all variables\n",
    "description=pd.DataFrame(description)\n",
    "topicality=pd.DataFrame(topicality)\n",
    "\n",
    "new_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\n",
    "description.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "new_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\n",
    "topicality.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "image_metadata = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_metadata['PetID'] = image_metadata['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# TOPICALITY #\n",
    "##############\n",
    "\n",
    "image_metadata['metadata_topicality_mean'] = image_metadata.iloc[:,1:10].mean(axis=1)\n",
    "image_metadata['metadata_topicality_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n",
    "\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.iloc[:,1:10].max(axis=1)\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n",
    "\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.iloc[:,1:10].min(axis=1)\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n",
    "\n",
    "\n",
    "image_metadata['metadata_topicality_0_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\n",
    "image_metadata['metadata_topicality_0_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(max)\n",
    "image_metadata['metadata_topicality_0_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n",
    "\n",
    "\n",
    "###############\n",
    "# DESCRIPTION #\n",
    "###############\n",
    "\n",
    "# Create Features from the Images\n",
    "image_metadata['L_metadata_0_cat']=image_metadata['metadata_description_0'].str.contains(\"cat\").astype(int)\n",
    "image_metadata['L_metadata_0_dog'] =image_metadata['metadata_description_0'].str.contains(\"dog\").astype(int)\n",
    "\n",
    "image_metadata['L_metadata_any_cat']=image_metadata.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\n",
    "image_metadata['L_metadata_any_dog']=image_metadata.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n",
    "\n",
    "image_metadata['L_metadata_0_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_cat'].transform('sum')\n",
    "image_metadata['L_metadata_0_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_dog'].transform('sum')\n",
    "\n",
    "image_metadata['L_metadata_any_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_cat'].transform('sum')\n",
    "image_metadata['L_metadata_any_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_dog'].transform('sum')\n",
    "\n",
    "image_metadata = image_metadata[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\n",
    "image_metadata=image_metadata.drop_duplicates('PetID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:43:09.910768Z",
     "start_time": "2019-03-02T16:42:53.658555Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = None  \n",
    "data = None  \n",
    "\n",
    "color_score_mean=[]\n",
    "color_score_min=[]\n",
    "color_score_max=[]\n",
    "\n",
    "color_pixelfrac_mean=[]\n",
    "color_pixelfrac_min=[]\n",
    "color_pixelfrac_max=[]\n",
    "\n",
    "imageid=[]\n",
    "\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          file_keys = list(d.keys())\n",
    "          if  'imagePropertiesAnnotation' in file_keys:\n",
    "              file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "              file_crops = d['cropHintsAnnotation']['cropHints']\n",
    "              \n",
    "              file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "              file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "              \n",
    "              file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n",
    "              file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n",
    "\n",
    "              \n",
    "              file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n",
    "              file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n",
    "              \n",
    "              \n",
    "          #Create a list with all image id name\n",
    "          imageid.append(filename.replace('.json',''))\n",
    "          \n",
    "          color_score_mean.append(file_color_score_mean)\n",
    "          color_score_min.append(file_color_score_min)\n",
    "          color_score_max.append(file_color_score_max)\n",
    "          \n",
    "          \n",
    "          color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n",
    "          color_pixelfrac_min.append(file_color_pixelfrac_min)\n",
    "          color_pixelfrac_max.append(file_color_pixelfrac_max)\n",
    "\n",
    "image_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# COLOR INFO #\n",
    "##############\n",
    "image_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \n",
    "image_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \n",
    "image_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n",
    "\n",
    "image_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \n",
    "image_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \n",
    "image_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max) \n",
    "\n",
    "image_properties=image_properties.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-02T16:41:37.334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_of_laplacian(data):\n",
    "    image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() \n",
    "\n",
    "blur=[]\n",
    "image_pixel=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\\\Users\\\\alorenzodebrionne\\\\Documents\\\\Kaggle\\\\Pet.my\\\\\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "              \n",
    "              #Blur \n",
    "              data = zfile.read(filename)\n",
    "              # Pixels\n",
    "              with Image.open( BytesIO(data)) as pixel:\n",
    "                  width, height = pixel.size\n",
    "              \n",
    "              pixel = width*height\n",
    "              \n",
    "              #image pixel size for each image\n",
    "              image_pixel.append(pixel)\n",
    "              #blur for each image\n",
    "              blur.append(variance_of_laplacian(data))\n",
    "              #image id\n",
    "              imageid.append(filename.replace('.jpg',''))\n",
    "          \n",
    "          \n",
    "# Join Pixel, Blur and Image ID\n",
    "image_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n",
    "                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n",
    "\n",
    "#Mean of the Mean\n",
    "image_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\n",
    "image_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n",
    "\n",
    "image_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \n",
    "image_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n",
    "\n",
    "image_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \n",
    "image_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n",
    "\n",
    "image_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\n",
    "image_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n",
    "\n",
    "\n",
    "image_quality = image_quality.drop(['blur','pixel'], 1)\n",
    "image_quality=image_quality.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-02T16:41:37.338Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "if name.value == \"train\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/train.csv')\n",
    "    \n",
    "    \n",
    "if name.value == \"test\":\n",
    "    df =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "\n",
    "#test = pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "breed =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\n",
    "color =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/color_labels.csv') #A pet could have multiple colors\n",
    "state =pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/state_labels.csv')\n",
    "\n",
    "# Add information about color, breed, state and sentiment data\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, state,  how='left', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_metadata,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "\n",
    "# Add information about quality Images\n",
    "df= (pd.merge(df, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-02T16:41:37.383Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n",
    "df['L_Color1'] = (pd.isnull(df['ColorName3']) & pd.isnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color2'] = (pd.isnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color3'] = (pd.notnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "\n",
    "# Breed (create a flag if the pet has 1 breed or 2)\n",
    "df['L_Breed1'] = (pd.isnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "df['L_Breed2'] = (pd.notnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "\n",
    "#Name (create a flag if the name is missing, with less than two letters)\n",
    "df['L_Name_missing'] =  (pd.isnull(df['Name'])).astype(int)\n",
    "df['Name_Length']=df['Name'].str.len() \n",
    "\n",
    "#Description \n",
    "df['Description_Length']=df['Description'].str.len() \n",
    "\n",
    "# Fee Amount\n",
    "df['L_Fee_Free'] =  (df['Fee']==0).astype(int)\n",
    "\n",
    "#Add the Number of Pets per Rescuer \n",
    "pets_total = df.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n",
    "df= pd.merge(df, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n",
    "df.count()\n",
    "\n",
    "# No photo\n",
    "df['L_NoPhoto'] =  (df['PhotoAmt']==0).astype(int)\n",
    "\n",
    "#No Video\n",
    "df['L_NoVideo'] =  (df['VideoAmt']==0).astype(int)\n",
    "\n",
    "#Log Age \n",
    "df['Log_Age']= np.log(df.Age+1) \n",
    "\n",
    "#Negative Score \n",
    "df['L_scoreneg'] =  (df['sentiment_document_score']<0).astype(int)\n",
    "\n",
    "#Quantity Amount >5\n",
    "df.loc[df['Quantity'] > 5, 'Quantity'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-02T16:41:37.391Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cannot be used for this analysis (IDs, Texts...)\n",
    "df = df.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n",
    "                            'BreedName1','Color1', 'Color2', 'Color3','Age','State'],axis=1)\n",
    "\n",
    "for col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n",
    "           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n",
    "           'blur_min','blur_max','blur_sum','blur_mean']:\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "    \n",
    "# replacing na values with No Color \n",
    "df[\"ColorName2\"].fillna(\"No Color\", inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-02T16:41:37.400Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T16:32:36.684460Z",
     "start_time": "2019-03-02T16:25:11.605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "df = pd.concat([df.drop('StateName', axis=1),pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "\n",
    "col=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated']\n",
    "for i in col:\n",
    "    df = pd.concat([df.drop(i, axis=1),pd.get_dummies(df[i], prefix=i)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output= name.value + \"features.csv\"\n",
    "filepath= os.path.join(\"C:\\Users\\alorenzo\\Documents\\Kaggle\\Pet.my\\\",output)\n",
    "df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
