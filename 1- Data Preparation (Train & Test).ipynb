{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://octodex.github.com/images/privateinvestocat.jpg\" alt=\"Kit\" title=\"Cat\" width=\"350\" height=\"200\" />\n",
    "*(image from octodex github)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Dataframe packages\n",
    "import zipfile\n",
    "import json\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Text Mining\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "\n",
    "#Widget package\n",
    "import ipywidgets as wg\n",
    "\n",
    "# Plot packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:57:06.560762Z",
     "start_time": "2019-03-05T21:57:06.491760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101cd3b10e074704a068973af0c58010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='train')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name= wg.Text(value='train')\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided by [Petfinder.my](https://www.kaggle.com/c/petfinder-adoption-prediction) , a platform dedicated for pets adoption. \n",
    "The objective is to predict at which speed a pet is adopted. \n",
    "\n",
    "There are 6 different sources of data + Images + Metadata Images and Sentiment Data \n",
    "-   Train.csv\n",
    "-   Test.csv\n",
    "-   breed_labels.csv\n",
    "-   color_labels.csv\n",
    "-   state_labels.csv\n",
    "-   Images (zip file) from cats and dogs that are adopted\n",
    "-   Metadata Images (zip file) information about the Image using Google Vision API\n",
    "-   Sentiment Data is based on the Descriptions using Google's Natural Language API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/i10SHmQ.jpg\" alt=\"K\" title=\"Datasets\" width=\"700\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T15:56:25.081214Z",
     "start_time": "2019-03-02T15:56:25.077198Z"
    }
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data Magnitude & Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:57:10.922829Z",
     "start_time": "2019-03-05T21:57:06.652768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    " \n",
    "score=[]\n",
    "magnitude=[]\n",
    "petid=[]\n",
    "\n",
    "string = name.value + \"_sentiment.zip\"\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z: #Read Zip File\n",
    "   for filename in z.namelist():  #Filename\n",
    "      with z.open(filename) as f:  #Open Filename\n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['documentSentiment'])\n",
    "         result_tuple = tuple(json_tree.execute('$..score')) #Object Value\n",
    "         result_tuple2=tuple(json_tree.execute('$..magnitude')) #Object Value\n",
    "         score.append(result_tuple)\n",
    "         magnitude.append(result_tuple2)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n",
    "                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data Entities Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define Empty lists\n",
    "entities=[]\n",
    "petid=[]\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['entities'])\n",
    "         result_tuple = tuple(json_tree.execute('$..name'))\n",
    " \n",
    "         entities.append(result_tuple)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "entities = pd.DataFrame(entities) #Transform to DataFrame\n",
    "entities['sentiment_entities_name'] = pd.Series(entities.fillna('').values.tolist()).str.join(' ') # Concatenate Columns\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_entities = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,entities.loc[:, ['sentiment_entities_name']]],axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will export the description and the topicality of each image. For more information on [Google API Vision](https://cloud.google.com/vision/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/7Hc9gXQ.png\" alt=\"Dogplant\" title=\"Dogplant\" width=\"500\" height=\"600\" />\n",
    "As example in this image the dog is not on the foreground. The Google API doesn't detect the dog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:58:14.678574Z",
     "start_time": "2019-03-05T21:57:10.926800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "description=[]\n",
    "topicality=[]\n",
    "imageid=[]\n",
    "\n",
    "\n",
    "string = name.value + \"_metadata.zip\"\n",
    "\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "# Read Zip File and Export a Dataset with the Topicality and Description for each Image\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "       #Open the Zip File\n",
    "      with z.open(filename) as f:  \n",
    "          #Read the json File\n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          #Check if the file contains the parent label Annotations and export description and topicality\n",
    "          if 'labelAnnotations' in d:\n",
    "             json_tree = objectpath.Tree(d['labelAnnotations'])\n",
    "             image_metadata1 = list(tuple(json_tree.execute('$..description')))\n",
    "             image_metadata2 =  list(tuple(json_tree.execute('$..topicality')))\n",
    "\n",
    "             #Create a list of all descriptions and topicality\n",
    "             description.append(image_metadata1)\n",
    "             topicality.append(image_metadata2)\n",
    "         \n",
    "             #Create a list with all image id name\n",
    "             imageid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "# Prepare the output by renaming all variables\n",
    "description=pd.DataFrame(description)\n",
    "topicality=pd.DataFrame(topicality)\n",
    "\n",
    "#Metadata Description of each image (10 descriptions maximum)\n",
    "new_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\n",
    "description.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "#Topicality of the image\n",
    "new_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\n",
    "topicality.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "image_metadata = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_metadata['PetID'] = image_metadata['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# TOPICALITY #\n",
    "##############\n",
    "\n",
    "#Average\n",
    "image_metadata['metadata_topicality_mean'] = image_metadata.iloc[:,1:10].mean(axis=1)\n",
    "image_metadata['metadata_topicality_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n",
    "\n",
    "#Max\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.iloc[:,1:10].max(axis=1)\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n",
    "\n",
    "#Minimum\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.iloc[:,1:10].min(axis=1)\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n",
    "\n",
    "#Group by Image\n",
    "image_metadata['metadata_topicality_0_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\n",
    "image_metadata['metadata_topicality_0_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(max)\n",
    "image_metadata['metadata_topicality_0_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n",
    "\n",
    "\n",
    "###############\n",
    "# DESCRIPTION #\n",
    "###############\n",
    "#print(image_metadata)\n",
    "\n",
    "# Create Features from the Images\n",
    "image_metadata['L_metadata_0_cat']=image_metadata['metadata_description_0'].str.contains(\"cat\").astype(int)\n",
    "image_metadata['L_metadata_0_dog'] =image_metadata['metadata_description_0'].str.contains(\"dog\").astype(int)\n",
    "\n",
    "image_metadata['L_metadata_any_cat']=image_metadata.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\n",
    "image_metadata['L_metadata_any_dog']=image_metadata.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n",
    "\n",
    "image_metadata['L_metadata_0_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_cat'].transform('sum')\n",
    "image_metadata['L_metadata_0_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_dog'].transform('sum')\n",
    "\n",
    "image_metadata['L_metadata_any_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_cat'].transform('sum')\n",
    "image_metadata['L_metadata_any_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_dog'].transform('sum')\n",
    "\n",
    "image_metadata = image_metadata.fillna(\"\")\n",
    "image_metadata['metadata_description']= image_metadata[['metadata_description_0', 'metadata_description_1',\n",
    "                                                        'metadata_description_2', 'metadata_description_3',\n",
    "                                                        'metadata_description_4', 'metadata_description_5',\n",
    "                                                        'metadata_description_6', 'metadata_description_7',\n",
    "                                                        'metadata_description_8', 'metadata_description_9']].apply(lambda x: ' '.join(x), axis=1)\n",
    "image_metadata = image_metadata[~image_metadata['ImageId'].str.endswith('-1')]\n",
    "image_metadata = image_metadata[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum',\n",
    "'metadata_description']]\n",
    "image_metadata=image_metadata.drop_duplicates('PetID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PetID', 'metadata_topicality_max', 'metadata_topicality_mean',\n",
       "       'metadata_topicality_min', 'metadata_topicality_0_mean',\n",
       "       'metadata_topicality_0_max', 'metadata_topicality_0_min',\n",
       "       'L_metadata_0_cat_sum', 'L_metadata_0_dog_sum',\n",
       "       'L_metadata_any_cat_sum', 'L_metadata_any_dog_sum',\n",
       "       'metadata_description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:58:28.843474Z",
     "start_time": "2019-03-05T21:58:14.683575Z"
    }
   },
   "outputs": [],
   "source": [
    "d = None  \n",
    "data = None  \n",
    "\n",
    "color_score_mean=[]\n",
    "color_score_min=[]\n",
    "color_score_max=[]\n",
    "\n",
    "color_pixelfrac_mean=[]\n",
    "color_pixelfrac_min=[]\n",
    "color_pixelfrac_max=[]\n",
    "\n",
    "crops=[]\n",
    "\n",
    "imageid=[]\n",
    "\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          file_keys = list(d.keys())\n",
    "          if  'imagePropertiesAnnotation' in file_keys:\n",
    "              file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "                        \n",
    "              file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "              file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "              \n",
    "              file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n",
    "              file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n",
    "\n",
    "              \n",
    "              file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n",
    "              file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n",
    "              \n",
    "              \n",
    "          #Create a list with all image id name\n",
    "          imageid.append(filename.replace('.json',''))\n",
    "          \n",
    "          color_score_mean.append(file_color_score_mean)\n",
    "          color_score_min.append(file_color_score_min)\n",
    "          color_score_max.append(file_color_score_max)\n",
    "          \n",
    "          \n",
    "          color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n",
    "          color_pixelfrac_min.append(file_color_pixelfrac_min)\n",
    "          color_pixelfrac_max.append(file_color_pixelfrac_max)\n",
    "\n",
    "        \n",
    "image_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# COLOR INFO #\n",
    "##############\n",
    "image_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \n",
    "image_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \n",
    "image_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n",
    "\n",
    "image_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \n",
    "image_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \n",
    "image_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max) \n",
    "\n",
    "image_properties=image_properties.drop_duplicates('PetID')\n",
    "image_properties = image_properties.drop(['ImageId'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image quality assessment aims to quantitatively represent the human perception of quality. To assign quality images we will add : pixels and  blur score using the variance of Laplacian.  \n",
    "The following variables are created:\n",
    "-  Pixel of all images for a pet\n",
    "-  Pixel average for all pictures for a Pet\n",
    "-  Blur of all images for a pet\n",
    "-  Blur average for all pictures for a Pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/KjM8TdF.jpg\" alt=\"CatBlur\" title=\"CatBlur\" width=\"200\" height=\"300\" />\n",
    "Example of a blur image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:35.822850Z",
     "start_time": "2019-03-05T21:58:28.846380Z"
    }
   },
   "outputs": [],
   "source": [
    "def variance_of_laplacian(data):\n",
    "    image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() \n",
    "\n",
    "blur=[]\n",
    "image_pixel=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "              \n",
    "              #Blur \n",
    "              data = zfile.read(filename)\n",
    "              # Pixels\n",
    "              with Image.open( BytesIO(data)) as pixel:\n",
    "                  width, height = pixel.size\n",
    "              \n",
    "              pixel = width*height\n",
    "              \n",
    "              #image pixel size for each image\n",
    "              image_pixel.append(pixel)\n",
    "              #blur for each image\n",
    "              blur.append(variance_of_laplacian(data))\n",
    "              #image id\n",
    "              imageid.append(filename.replace('.jpg',''))\n",
    "          \n",
    "          \n",
    "# Join Pixel, Blur and Image ID\n",
    "image_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n",
    "                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n",
    "# create the PetId variable\n",
    "image_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n",
    "\n",
    "#Mean of the Mean\n",
    "image1 = image_quality[image_quality['ImageId'].str.endswith('-1')]\n",
    "image1.columns=['ImageId_1','blur_1','pixel_1','PetID1']\n",
    "\n",
    "image_quality = image_quality.merge(image1,left_on='PetID', right_on='PetID1')\n",
    "\n",
    "image_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\n",
    "image_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n",
    "\n",
    "image_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \n",
    "image_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n",
    "\n",
    "image_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \n",
    "image_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n",
    "\n",
    "image_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\n",
    "image_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n",
    "\n",
    "\n",
    "image_quality = image_quality.drop(['blur','pixel','ImageId','ImageId_1'], 1)\n",
    "image_quality=image_quality.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image as grayscale image\n",
    "from math import copysign, log10\n",
    "import numpy as np\n",
    "\n",
    "huMoments0=[]\n",
    "huMoments1=[]\n",
    "huMoments2=[]\n",
    "huMoments3=[]\n",
    "huMoments4=[]\n",
    "huMoments5=[]\n",
    "huMoments6=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "            data = zfile.read(filename)\n",
    "            \n",
    "            image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "            im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "            # Calculate Moments\n",
    "            moments = cv2.moments(im)\n",
    " \n",
    "            # Calculate Hu Moments\n",
    "            huMoments = cv2.HuMoments(moments)\n",
    "            # Log scale hu moments\n",
    "            for i in range(0,7):\n",
    "                  huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n",
    "                \n",
    "            #image id\n",
    "            imageid.append(filename.replace('.jpg',''))\n",
    "            huMoments0.append(huMoments[0])\n",
    "            \n",
    "            huMoments1.append(huMoments[1])\n",
    "            huMoments2.append(huMoments[2])\n",
    "            huMoments3.append(huMoments[3])\n",
    "            huMoments4.append(huMoments[4])\n",
    "            huMoments5.append(huMoments[5])\n",
    "            huMoments6.append(huMoments[6])\n",
    "\n",
    "image_moments = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n",
    "                           pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n",
    "                           pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n",
    "\n",
    "arr = np.array(data)\n",
    "\n",
    "# create the PetId variable\n",
    "image_moments['PetID'] = image_moments['ImageId'].str.split('-').str[0]\n",
    "image_moments = image_moments[image_moments['ImageId'].apply(lambda x:x.endswith((\"-1\")))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.494880Z",
     "start_time": "2019-03-05T22:04:35.824763Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "if name.value == \"train\":\n",
    "    df =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\train.csv')\n",
    "    \n",
    "    \n",
    "if name.value == \"test\":\n",
    "    df =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\test.csv')\n",
    "\n",
    "#test = pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "breed =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\n",
    "color =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\color_labels.csv') #A pet could have multiple colors\n",
    "state =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\state_labels.csv')\n",
    "\n",
    "# Add information about color, breed, state and sentiment data\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, state,  how='left', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "df = (pd.merge(df, sentimental_entities,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_metadata,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "df = (pd.merge(df, image_moments,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about quality Images\n",
    "df= (pd.merge(df, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.559881Z",
     "start_time": "2019-03-05T22:04:42.496877Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malaysia Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.624875Z",
     "start_time": "2019-03-05T22:04:42.562882Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area ={\n",
    "    41336:19102,\n",
    "41325:9500,\n",
    "41367:15099,\n",
    "41401:243,\n",
    "41415:91,\n",
    "41324:1664,\n",
    "41332:6686,\n",
    "41335:36137,\n",
    "41330:21035,\n",
    "41380:821,\n",
    "41327:1048,\n",
    "41345:73631,\n",
    "41342:124450,\n",
    "41326:8104,\n",
    "41361:13035}\n",
    "\n",
    "state_unemployment ={\n",
    "    41336 : 3.6,\n",
    "41325 :2.9,\n",
    "41367: 3.8,\n",
    "41324: 0.9,\n",
    "41332 : 2.7,\n",
    "41335: 2.6,\n",
    "41330: 3.4,\n",
    "41380: 2.9,\n",
    "41327: 2.1,\n",
    "41345 : 5.4,\n",
    "41342 : 3.3,\n",
    "41326: 3.2,\n",
    "41361: 4.2,\n",
    "41415: 7.8,\n",
    "41401: 3.3\n",
    "}\n",
    "\n",
    "# per 1000 population\n",
    "state_birth_rate = {\n",
    " 41336:16.3,\n",
    "41325:17.0,\n",
    "41367:21.4,\n",
    "41401:14.4,\n",
    "41415:18.1,\n",
    "41324:16.0,\n",
    "41332:16.4,\n",
    "41335:17.0,\n",
    "41330:14.4,\n",
    "41380:17.5,\n",
    "41327:12.7,\n",
    "41345:13.7,\n",
    "41342:13.9,\n",
    "41326:16.6,\n",
    "41361:23.3,     \n",
    "}\n",
    "\n",
    "df[\"state_gdp\"] = df.State.map(state_gdp)\n",
    "df[\"state_population\"] = df.State.map(state_population)\n",
    "df[\"state_area\"] = df.State.map(state_area)\n",
    "df['state_unemployment']=df.State.map(state_unemployment)\n",
    "df['state_birth_rate']=df.State.map(state_birth_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:43.042880Z",
     "start_time": "2019-03-05T22:04:42.629882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n",
    "df['L_Color1'] = (pd.isnull(df['ColorName3']) & pd.isnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color2'] = (pd.isnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color3'] = (pd.notnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "\n",
    "# Breed (create a flag if the pet has 1 breed or 2)\n",
    "df['L_Breed1'] = (pd.isnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "df['L_Breed2'] = (pd.notnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "\n",
    "# Breed create columns\n",
    "df['L_Breed1_Siamese'] =(df['BreedName1']=='Siamese').astype(int)\n",
    "df['L_Breed1_Persian']=(df['BreedName1']=='Persian').astype(int)\n",
    "df['L_Breed1_Labrador_Retriever']=(df['BreedName1']=='Labrador Retriever').astype(int)\n",
    "df['L_Breed1_Terrier']=(df['BreedName1']=='Terrier').astype(int)\n",
    "df['L_Breed1_Golden_Retriever ']=(df['BreedName1']=='Golden Retriever').astype(int)\n",
    "\n",
    "#Name (create a flag if the name is missing, with less than two letters)\n",
    "df['L_Name_missing'] =  (pd.isnull(df['Name'])).astype(int)\n",
    "df['Name_Length']=df['Name'].str.len() \n",
    "\n",
    "#Description \n",
    "df['Description_Length']=df['Description'].str.len() \n",
    "\n",
    "# Fee Amount\n",
    "df['L_Fee_Free'] =  (df['Fee']==0).astype(int)\n",
    "\n",
    "#Add the Number of Pets per Rescuer \n",
    "pets_total = df.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n",
    "df= pd.merge(df, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n",
    "df.count()\n",
    "\n",
    "# No photo\n",
    "df['L_NoPhoto'] =  (df['PhotoAmt']==0).astype(int)\n",
    "\n",
    "#No Video\n",
    "df['L_NoVideo'] =  (df['VideoAmt']==0).astype(int)\n",
    "\n",
    "#Log Age \n",
    "df['Log_Age']= np.log(df.Age+1) \n",
    "\n",
    "#Negative Score \n",
    "df['L_scoreneg'] =  (df['sentiment_document_score']<0).astype(int)\n",
    "\n",
    "#Quantity Amount >5\n",
    "df.loc[df['Quantity'] > 5, 'Quantity'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:04.540006Z",
     "start_time": "2019-03-05T22:04:43.045884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the Variable Description\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cleaning_text(df,column):  \n",
    "    df[column] =df[column].fillna(\"<MISSING>\") #Replace missing values\n",
    "    df[column] = df[column].str.replace('\\d+', '') #Remove numbers\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace('[^\\w\\s]','') #Remove punctuactions\n",
    "    df[column] = df[column].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ') #Delete duplicated word\n",
    "\n",
    "    stop = stopwords.words('english') #Do not take into account stop words (ex: The)\n",
    "\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df[column] = df[column].str.replace(pat, '')\n",
    "    df[column] = df[column].str.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Stem Words\n",
    "    df[column] = df[column].astype(str).str.split()\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    df[column]=df[column].apply(lambda x : [porter_stemmer.stem(y) for y in x]) #Word of the same family\n",
    "\n",
    "    df[column]=df[column].apply(lambda x : \" \".join(x))\n",
    "    return df\n",
    "\n",
    "df = cleaning_text(df,'Description')\n",
    "df = cleaning_text(df,'sentiment_entities_name')\n",
    "df = cleaning_text (df,'metadata_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.551811Z",
     "start_time": "2019-03-05T22:05:04.541990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix Factorization for dimensionality reduction\n",
    "def matrix_factorization(df,column,prefix_svd, prefix_nmf):\n",
    "    \n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=5, random_state=1337)\n",
    "    nmf_ = NMF(\n",
    "        n_components=5, random_state=1337)\n",
    "\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(df[column])\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix(prefix_svd)\n",
    "\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix(prefix_nmf)\n",
    "    df = pd.concat([df,nmf_col,svd_col],axis=1)\n",
    "    return df\n",
    "    \n",
    "df = matrix_factorization(df,'Description','SVD_Desc_','NMF_Desc_')\n",
    "df = matrix_factorization(df,'sentiment_entities_name','SVD_entities_','NMF_entities_')\n",
    "df = matrix_factorization(df,'metadata_description','SVD_metadata_desc_','NMF_metadata_desc_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.587813Z",
     "start_time": "2019-03-05T22:05:06.553815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cannot be used for this analysis (IDs, Texts...)\n",
    "#df = df.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID',#'Description',\n",
    "#              'BreedName1','Color1', 'Color2', 'Color3','Age','State','metadata_description','sentiment_entities_name'\n",
    "#              'ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength','StateName'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "\n",
    "missing_value_df =missing_value_df[missing_value_df['percent_missing']>0]\n",
    "missing_value_df\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x=\"column_name\", y=\"percent_missing\", data=missing_value_df, label='Sales')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=75)\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % round(p.get_height(),2) + '%', \n",
    "            fontsize=12, color='grey', ha='center', va='bottom') \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.627814Z",
     "start_time": "2019-03-05T22:05:06.588811Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n",
    "           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n",
    "           'blur_min','blur_max','blur_sum','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n",
    "           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n",
    "            'Name_Length','Description_Length','huMoments0','huMoments1','huMoments2','huMoments3','huMoments4','huMoments5','huMoments6']:\n",
    "    \n",
    "\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "    \n",
    "# replacing na values with No Color \n",
    "df[\"ColorName2\"].fillna(\"No Color\", inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:07.026816Z",
     "start_time": "2019-03-05T22:05:06.630816Z"
    }
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "#df = pd.concat([df.drop('StateName', axis=1),pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "\n",
    "col=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\n",
    "for i in col:\n",
    "    #df = pd.concat([df.drop(i, axis=1),pd.get_dummies(df[i], prefix=i)], axis=1)\n",
    "    df = pd.concat([df,pd.get_dummies(df[i], prefix=i)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= name.value + \"_features.csv\"\n",
    "print(output)\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", output)\n",
    "df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "712px",
    "left": "1622px",
    "top": "111.133px",
    "width": "200px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.233334,
   "position": {
    "height": "553.233px",
    "left": "1533px",
    "right": "37px",
    "top": "163px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
