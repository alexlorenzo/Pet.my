{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://octodex.github.com/images/privateinvestocat.jpg\" alt=\"Kit\" title=\"Cat\" width=\"350\" height=\"200\" />\n",
    "*(image from octodex github)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Dataframe packages\n",
    "import zipfile\n",
    "import json\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Text Mining\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "\n",
    "#Widget package\n",
    "import ipywidgets as wg\n",
    "\n",
    "# Plot packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:57:06.560762Z",
     "start_time": "2019-03-05T21:57:06.491760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101cd3b10e074704a068973af0c58010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='train')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name= wg.Text(value='train')\n",
    "display(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided by [Petfinder.my](https://www.kaggle.com/c/petfinder-adoption-prediction) , a platform dedicated for pets adoption. \n",
    "The objective is to predict at which speed a pet is adopted. \n",
    "\n",
    "There are 6 different sources of data + Images + Metadata Images and Sentiment Data \n",
    "-   Train.csv\n",
    "-   Test.csv\n",
    "-   breed_labels.csv\n",
    "-   color_labels.csv\n",
    "-   state_labels.csv\n",
    "-   Images (zip file) from cats and dogs that are adopted\n",
    "-   Metadata Images (zip file) information about the Image using Google Vision API\n",
    "-   Sentiment Data is based on the Descriptions using Google's Natural Language API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/i10SHmQ.jpg\" alt=\"K\" title=\"Datasets\" width=\"700\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T15:56:25.081214Z",
     "start_time": "2019-03-02T15:56:25.077198Z"
    }
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data Magnitude & Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:57:10.922829Z",
     "start_time": "2019-03-05T21:57:06.652768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    " \n",
    "score=[]\n",
    "magnitude=[]\n",
    "petid=[]\n",
    "\n",
    "string = name.value + \"_sentiment.zip\"\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z: #Read Zip File\n",
    "   for filename in z.namelist():  #Filename\n",
    "      with z.open(filename) as f:  #Open Filename\n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['documentSentiment'])\n",
    "         result_tuple = tuple(json_tree.execute('$..score')) #Object Value\n",
    "         result_tuple2=tuple(json_tree.execute('$..magnitude')) #Object Value\n",
    "         score.append(result_tuple)\n",
    "         magnitude.append(result_tuple2)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_analysis = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n",
    "                                                pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Data Entities Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define Empty lists\n",
    "entities=[]\n",
    "petid=[]\n",
    "\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "         data = f.read()  \n",
    "         d = json.loads(data.decode(\"utf-8\"))\n",
    "         json_tree = objectpath.Tree(d['entities'])\n",
    "         result_tuple = tuple(json_tree.execute('$..name'))\n",
    " \n",
    "         entities.append(result_tuple)\n",
    "         \n",
    "      petid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "entities = pd.DataFrame(entities) #Transform to DataFrame\n",
    "entities['sentiment_entities_name'] = pd.Series(entities.fillna('').values.tolist()).str.join(' ') # Concatenate Columns\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "sentimental_entities = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,entities.loc[:, ['sentiment_entities_name']]],axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will export the description and the topicality of each image. For more information on [Google API Vision](https://cloud.google.com/vision/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/7Hc9gXQ.png\" alt=\"Dogplant\" title=\"Dogplant\" width=\"500\" height=\"600\" />\n",
    "As example in this image the dog is not on the foreground. The Google API doesn't detect the dog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:58:14.678574Z",
     "start_time": "2019-03-05T21:57:10.926800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Empty lists\n",
    "d = None  \n",
    "data = None  \n",
    "description=[]\n",
    "topicality=[]\n",
    "imageid=[]\n",
    "\n",
    "\n",
    "string = name.value + \"_metadata.zip\"\n",
    "\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "# Read Zip File and Export a Dataset with the Topicality and Description for each Image\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "       #Open the Zip File\n",
    "      with z.open(filename) as f:  \n",
    "          #Read the json File\n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          #Check if the file contains the parent label Annotations and export description and topicality\n",
    "          if 'labelAnnotations' in d:\n",
    "             json_tree = objectpath.Tree(d['labelAnnotations'])\n",
    "             image_metadata1 = list(tuple(json_tree.execute('$..description')))\n",
    "             image_metadata2 =  list(tuple(json_tree.execute('$..topicality')))\n",
    "\n",
    "             #Create a list of all descriptions and topicality\n",
    "             description.append(image_metadata1)\n",
    "             topicality.append(image_metadata2)\n",
    "         \n",
    "             #Create a list with all image id name\n",
    "             imageid.append(filename.replace('.json',''))\n",
    "\n",
    "\n",
    "# Prepare the output by renaming all variables\n",
    "description=pd.DataFrame(description)\n",
    "topicality=pd.DataFrame(topicality)\n",
    "\n",
    "#Metadata Description of each image (10 descriptions maximum)\n",
    "new_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\n",
    "description.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "#Topicality of the image\n",
    "new_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\n",
    "topicality.rename(columns = dict(new_names), inplace=True)\n",
    "\n",
    "# Output with sentiment data for each pet\n",
    "image_metadata = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_metadata['PetID'] = image_metadata['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# TOPICALITY #\n",
    "##############\n",
    "\n",
    "#Average\n",
    "image_metadata['metadata_topicality_mean'] = image_metadata.iloc[:,1:10].mean(axis=1)\n",
    "image_metadata['metadata_topicality_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n",
    "\n",
    "#Max\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.iloc[:,1:10].max(axis=1)\n",
    "image_metadata['metadata_topicality_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n",
    "\n",
    "#Minimum\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.iloc[:,1:10].min(axis=1)\n",
    "image_metadata['metadata_topicality_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n",
    "\n",
    "#Group by Image\n",
    "image_metadata['metadata_topicality_0_mean']  = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\n",
    "image_metadata['metadata_topicality_0_max'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(max)\n",
    "image_metadata['metadata_topicality_0_min'] = image_metadata.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n",
    "\n",
    "\n",
    "###############\n",
    "# DESCRIPTION #\n",
    "###############\n",
    "#print(image_metadata)\n",
    "\n",
    "# Create Features from the Images\n",
    "image_metadata['L_metadata_0_cat']=image_metadata['metadata_description_0'].str.contains(\"cat\").astype(int)\n",
    "image_metadata['L_metadata_0_dog'] =image_metadata['metadata_description_0'].str.contains(\"dog\").astype(int)\n",
    "\n",
    "image_metadata['L_metadata_any_cat']=image_metadata.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\n",
    "image_metadata['L_metadata_any_dog']=image_metadata.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n",
    "\n",
    "image_metadata['L_metadata_0_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_cat'].transform('sum')\n",
    "image_metadata['L_metadata_0_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_0_dog'].transform('sum')\n",
    "\n",
    "image_metadata['L_metadata_any_cat_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_cat'].transform('sum')\n",
    "image_metadata['L_metadata_any_dog_sum'] = image_metadata.groupby(image_metadata['PetID'])['L_metadata_any_dog'].transform('sum')\n",
    "\n",
    "image_metadata = image_metadata.fillna(\"\")\n",
    "image_metadata['metadata_description']= image_metadata[['metadata_description_0', 'metadata_description_1',\n",
    "                                                        'metadata_description_2', 'metadata_description_3',\n",
    "                                                        'metadata_description_4', 'metadata_description_5',\n",
    "                                                        'metadata_description_6', 'metadata_description_7',\n",
    "                                                        'metadata_description_8', 'metadata_description_9']].apply(lambda x: ' '.join(x), axis=1)\n",
    "image_metadata = image_metadata[~image_metadata['ImageId'].str.endswith('-1')]\n",
    "image_metadata = image_metadata[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum',\n",
    "'metadata_description']]\n",
    "image_metadata=image_metadata.drop_duplicates('PetID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PetID', 'metadata_topicality_max', 'metadata_topicality_mean',\n",
       "       'metadata_topicality_min', 'metadata_topicality_0_mean',\n",
       "       'metadata_topicality_0_max', 'metadata_topicality_0_min',\n",
       "       'L_metadata_0_cat_sum', 'L_metadata_0_dog_sum',\n",
       "       'L_metadata_any_cat_sum', 'L_metadata_any_dog_sum',\n",
       "       'metadata_description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T21:58:28.843474Z",
     "start_time": "2019-03-05T21:58:14.683575Z"
    }
   },
   "outputs": [],
   "source": [
    "d = None  \n",
    "data = None  \n",
    "\n",
    "color_score_mean=[]\n",
    "color_score_min=[]\n",
    "color_score_max=[]\n",
    "\n",
    "color_pixelfrac_mean=[]\n",
    "color_pixelfrac_min=[]\n",
    "color_pixelfrac_max=[]\n",
    "\n",
    "crops=[]\n",
    "\n",
    "imageid=[]\n",
    "\n",
    "# Read Zip File and Export a Dataset with the Score and the ID\n",
    "with zipfile.ZipFile(filepath, \"r\") as z:\n",
    "   for filename in z.namelist():  \n",
    "      with z.open(filename) as f:  \n",
    "          data = f.read()\n",
    "          d = json.loads(data.decode(\"utf-8\"))\n",
    "          file_keys = list(d.keys())\n",
    "          if  'imagePropertiesAnnotation' in file_keys:\n",
    "              file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "                        \n",
    "              file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "              file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "              \n",
    "              file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n",
    "              file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n",
    "\n",
    "              \n",
    "              file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n",
    "              file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n",
    "              \n",
    "              \n",
    "          #Create a list with all image id name\n",
    "          imageid.append(filename.replace('.json',''))\n",
    "          \n",
    "          color_score_mean.append(file_color_score_mean)\n",
    "          color_score_min.append(file_color_score_min)\n",
    "          color_score_max.append(file_color_score_max)\n",
    "          \n",
    "          \n",
    "          color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n",
    "          color_pixelfrac_min.append(file_color_pixelfrac_min)\n",
    "          color_pixelfrac_max.append(file_color_pixelfrac_max)\n",
    "\n",
    "        \n",
    "image_properties = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n",
    "\n",
    "# create the PetId variable\n",
    "image_properties['PetID'] = image_properties['ImageId'].str.split('-').str[0]\n",
    "\n",
    "\n",
    "##############\n",
    "# COLOR INFO #\n",
    "##############\n",
    "image_properties['metadata_color_pixelfrac_mean']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \n",
    "image_properties['metadata_color_pixelfrac_min']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \n",
    "image_properties['metadata_color_pixelfrac_max']  = image_properties.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n",
    "\n",
    "image_properties['metadata_color_score_mean']  = image_properties.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \n",
    "image_properties['metadata_color_score_min']  = image_properties.groupby(['PetID'])['metadata_color_score_min'].transform(min) \n",
    "image_properties['metadata_color_score_max']  = image_properties.groupby(['PetID'])['metadata_color_score_max'].transform(max) \n",
    "\n",
    "image_properties=image_properties.drop_duplicates('PetID')\n",
    "image_properties = image_properties.drop(['ImageId'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image quality assessment aims to quantitatively represent the human perception of quality. To assign quality images we will add : pixels and  blur score using the variance of Laplacian.  \n",
    "The following variables are created:\n",
    "-  Pixel of all images for a pet\n",
    "-  Pixel average for all pictures for a Pet\n",
    "-  Blur of all images for a pet\n",
    "-  Blur average for all pictures for a Pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/KjM8TdF.jpg\" alt=\"CatBlur\" title=\"CatBlur\" width=\"200\" height=\"300\" />\n",
    "Example of a blur image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:35.822850Z",
     "start_time": "2019-03-05T21:58:28.846380Z"
    }
   },
   "outputs": [],
   "source": [
    "def variance_of_laplacian(data):\n",
    "    image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() \n",
    "\n",
    "blur=[]\n",
    "image_pixel=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "              \n",
    "              #Blur \n",
    "              data = zfile.read(filename)\n",
    "              # Pixels\n",
    "              with Image.open( BytesIO(data)) as pixel:\n",
    "                  width, height = pixel.size\n",
    "              \n",
    "              pixel = width*height\n",
    "              \n",
    "              #image pixel size for each image\n",
    "              image_pixel.append(pixel)\n",
    "              #blur for each image\n",
    "              blur.append(variance_of_laplacian(data))\n",
    "              #image id\n",
    "              imageid.append(filename.replace('.jpg',''))\n",
    "          \n",
    "          \n",
    "# Join Pixel, Blur and Image ID\n",
    "image_quality = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n",
    "                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n",
    "# create the PetId variable\n",
    "image_quality['PetID'] = image_quality['ImageId'].str.split('-').str[0]\n",
    "\n",
    "#Mean of the Mean\n",
    "image1 = image_quality[image_quality['ImageId'].str.endswith('-1')]\n",
    "image1.columns=['ImageId_1','blur_1','pixel_1','PetID1']\n",
    "\n",
    "image_quality = image_quality.merge(image1,left_on='PetID', right_on='PetID1')\n",
    "\n",
    "image_quality['pixel_mean'] = image_quality.groupby(['PetID'])['pixel'].transform('mean')\n",
    "image_quality['blur_mean'] = image_quality.groupby(['PetID'])['blur'].transform('mean') \n",
    "\n",
    "image_quality['pixel_min'] = image_quality.groupby(['PetID'])['pixel'].transform('min') \n",
    "image_quality['blur_min'] = image_quality.groupby(['PetID'])['blur'].transform('min')\n",
    "\n",
    "image_quality['pixel_max'] = image_quality.groupby(['PetID'])['pixel'].transform('max') \n",
    "image_quality['blur_max'] = image_quality.groupby(['PetID'])['blur'].transform('max')\n",
    "\n",
    "image_quality['pixel_sum'] = image_quality.groupby(['PetID'])['pixel'].transform('sum')\n",
    "image_quality['blur_sum'] = image_quality.groupby(['PetID'])['blur'].transform('sum')\n",
    "\n",
    "\n",
    "image_quality = image_quality.drop(['blur','pixel','ImageId','ImageId_1'], 1)\n",
    "image_quality=image_quality.drop_duplicates('PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           PetID       blur_1  pixel_1     PetID1     pixel_mean    blur_mean  \\\n",
      "0      de993d8ad   789.829266   120000  de993d8ad  120000.000000   481.739561   \n",
      "3      b7142b615   341.153974   120000  b7142b615  120000.000000   304.461764   \n",
      "11     d6c60a52d    61.693209    20584  d6c60a52d   25951.333333   288.914944   \n",
      "14     0f6c2d090   167.312757   106400  0f6c2d090  115090.909091   240.007081   \n",
      "25     e6bc32c73  1326.391723   120000  e6bc32c73  120000.000000  1710.787136   \n",
      "28     4d8bf7feb  1228.191717   120000  4d8bf7feb  120000.000000  1153.183411   \n",
      "31     4732c7563   317.842668   172800  4732c7563  172800.000000   295.648802   \n",
      "35     9d466bbca   294.486610   120000  9d466bbca  115408.909091   604.807373   \n",
      "46     fe3fe601e  1640.025935   120000  fe3fe601e  120100.000000  1469.094943   \n",
      "50     d51b2e785   213.602437   120000  d51b2e785  120000.000000   236.029526   \n",
      "55     e7c607873   121.293109   120000  e7c607873  120000.000000   134.579769   \n",
      "59     c1ca516d7   697.443648   172800  c1ca516d7  195696.000000   438.484538   \n",
      "69     d120df077   334.587067   129600  d120df077  134619.428571   573.196995   \n",
      "76     9b6b350c8   463.799525    89320  9b6b350c8   89320.000000   366.751895   \n",
      "78     742462799  3159.600683   172800  742462799  192576.000000  3797.365542   \n",
      "98     a86f09670   844.124501   182880  a86f09670  176160.000000   682.823025   \n",
      "103    78ccb8039  1039.209731   120000  78ccb8039  120000.000000   624.368815   \n",
      "113    ce5e6e407  1902.082593   230040  ce5e6e407  230040.000000  2186.359908   \n",
      "118    ec0fd1a7e    73.352418   113568  ec0fd1a7e  116784.000000    87.412982   \n",
      "120    ccf8c71f6    47.127282   120000  ccf8c71f6  120000.000000   103.867015   \n",
      "131    642a88a93   148.022456    96000  642a88a93   96000.000000   128.389298   \n",
      "134    274c7f123   397.578487   172800  274c7f123  239040.000000   410.818578   \n",
      "138    6dbb13673    63.193927   172800  6dbb13673  262400.000000    89.471014   \n",
      "141    3cdbf07be    80.605125   120000  3cdbf07be  120000.000000    84.478022   \n",
      "146    2db00a064  1980.808965    97600  2db00a064  126800.000000  1207.424697   \n",
      "148    57f322839  4658.452276   165600  57f322839  173760.000000  2136.192274   \n",
      "154    e2b5913e7  1879.560613   166680  e2b5913e7  235293.333333  1447.864955   \n",
      "157    21493e6ea  1428.860436   172800  21493e6ea  172800.000000  1093.454268   \n",
      "165    e3a372f31   826.017132   120000  e3a372f31  120000.000000   826.017132   \n",
      "166    e58bebf13  1444.973668   133600  e58bebf13  120016.714286  1089.987211   \n",
      "...          ...          ...      ...        ...            ...          ...   \n",
      "58281  76c3b16af   551.394874   172800  76c3b16af  172800.000000   551.394874   \n",
      "58282  7b01ae317  1041.103197   409600  7b01ae317  409600.000000  1041.103197   \n",
      "58283  c5422c04b   653.349913   172800  c5422c04b  172800.000000   653.349913   \n",
      "58284  141b5d279  1011.451891   109600  141b5d279  109600.000000  1011.451891   \n",
      "58285  b6256e92e  4649.885701    50325  b6256e92e   50325.000000  4649.885701   \n",
      "58286  0eba8b0ad   768.365754   172800  0eba8b0ad  172800.000000   768.365754   \n",
      "58287  fdfa41872   610.308642   230400  fdfa41872  230400.000000   610.308642   \n",
      "58288  43c623f94  2608.044153   230400  43c623f94  230400.000000  2608.044153   \n",
      "58289  1e753137d   984.500109   307200  1e753137d  307200.000000   984.500109   \n",
      "58290  1bbb46822   986.932739   110187  1bbb46822  110187.000000   986.932739   \n",
      "58291  d4850786d   423.533488   172800  d4850786d  172800.000000   423.533488   \n",
      "58292  4fe9ef7ce  2327.543573    36400  4fe9ef7ce   36400.000000  2327.543573   \n",
      "58293  f618c35b8    83.859233   120000  f618c35b8  120000.000000    83.859233   \n",
      "58294  77bff1468  3552.199833   116400  77bff1468  116400.000000  3552.199833   \n",
      "58295  1c723cf91  1631.107136   120000  1c723cf91  120000.000000  1631.107136   \n",
      "58296  90fb6e125   216.246692   120000  90fb6e125  120000.000000   216.246692   \n",
      "58297  dcf366e02   198.197740   120000  dcf366e02  120000.000000   198.197740   \n",
      "58298  25b827551   385.470616    95600  25b827551   95600.000000   385.470616   \n",
      "58299  682b7b7d0  2284.282272    80150  682b7b7d0   80150.000000  2284.282272   \n",
      "58300  838b5c426   597.592662   172800  838b5c426  172800.000000   597.592662   \n",
      "58301  618e97eeb   172.161451   120000  618e97eeb  120000.000000   172.161451   \n",
      "58302  36d7fc2c5   702.330642   119200  36d7fc2c5  119200.000000   702.330642   \n",
      "58303  d4427513d   523.702058   120000  d4427513d  120000.000000   523.702058   \n",
      "58304  53cfa1a3c   407.062052   172800  53cfa1a3c  172800.000000   407.062052   \n",
      "58305  44bc5f67c   137.276395   120000  44bc5f67c  120000.000000   137.276395   \n",
      "58306  01d646ea8   647.429179   112565  01d646ea8  112565.000000   647.429179   \n",
      "58307  6d94a56ef   321.547597   172800  6d94a56ef  172800.000000   321.547597   \n",
      "58308  d755e40d7   150.243844    90000  d755e40d7   90000.000000   150.243844   \n",
      "58309  8102d6dc7   163.615212   120000  8102d6dc7  120000.000000   163.615212   \n",
      "58310  9304f9f8b   728.079288   120000  9304f9f8b  120000.000000   728.079288   \n",
      "\n",
      "       pixel_min     blur_min  pixel_max      blur_max  pixel_sum  \\\n",
      "0         120000   128.048366     120000    789.829266     360000   \n",
      "3         120000    97.838731     120000    558.737106     960000   \n",
      "11         20584    61.693209      36686    713.851425      77854   \n",
      "14        106400   106.698012     160000    452.109717    1266000   \n",
      "25        120000  1326.391723     120000   2123.093430     360000   \n",
      "28        120000   358.184705     120000   1873.173811     360000   \n",
      "31        172800   144.273267     172800    493.763747     691200   \n",
      "35         50298    57.182160     139200   1844.816799    1269498   \n",
      "46        120000  1190.228102     120400   1640.025935     480400   \n",
      "50        120000    80.789945     120000    400.543212     600000   \n",
      "55        120000    40.635003     120000    292.359122     480000   \n",
      "59        172800   109.366898     230040    708.608419    1956960   \n",
      "69        129600   172.101052     147456    993.076153     942336   \n",
      "76         89320   269.704265      89320    463.799525     178640   \n",
      "78        129600   250.966608     385920  11812.902221    3851520   \n",
      "98        172800   376.515578     182880    844.124501     880800   \n",
      "103       120000    87.601967     120000   1595.729350    1200000   \n",
      "113       230040  1832.033235     230040   3159.378814    1150200   \n",
      "118       113568    73.352418     120000    101.473545     233568   \n",
      "120       120000    37.236363     120000    369.211875    1320000   \n",
      "131        96000   117.953323      96000    148.022456     288000   \n",
      "134       172800   330.944879     316160    460.368101     956160   \n",
      "138       172800    63.193927     307200    141.092699     787200   \n",
      "141       120000    64.332015     120000     96.512994     600000   \n",
      "146        97600   434.040428     156000   1980.808965     253600   \n",
      "148       165600    74.851236     185760   4658.452276    1042560   \n",
      "154       129600   841.401733     409600   1879.560613     705880   \n",
      "157       172800   658.599223     172800   1828.108798    1382400   \n",
      "165       120000   826.017132     120000    826.017132     120000   \n",
      "166        98800   716.324581     134000   1820.428436     840117   \n",
      "...          ...          ...        ...           ...        ...   \n",
      "58281     172800   551.394874     172800    551.394874     172800   \n",
      "58282     409600  1041.103197     409600   1041.103197     409600   \n",
      "58283     172800   653.349913     172800    653.349913     172800   \n",
      "58284     109600  1011.451891     109600   1011.451891     109600   \n",
      "58285      50325  4649.885701      50325   4649.885701      50325   \n",
      "58286     172800   768.365754     172800    768.365754     172800   \n",
      "58287     230400   610.308642     230400    610.308642     230400   \n",
      "58288     230400  2608.044153     230400   2608.044153     230400   \n",
      "58289     307200   984.500109     307200    984.500109     307200   \n",
      "58290     110187   986.932739     110187    986.932739     110187   \n",
      "58291     172800   423.533488     172800    423.533488     172800   \n",
      "58292      36400  2327.543573      36400   2327.543573      36400   \n",
      "58293     120000    83.859233     120000     83.859233     120000   \n",
      "58294     116400  3552.199833     116400   3552.199833     116400   \n",
      "58295     120000  1631.107136     120000   1631.107136     120000   \n",
      "58296     120000   216.246692     120000    216.246692     120000   \n",
      "58297     120000   198.197740     120000    198.197740     120000   \n",
      "58298      95600   385.470616      95600    385.470616      95600   \n",
      "58299      80150  2284.282272      80150   2284.282272      80150   \n",
      "58300     172800   597.592662     172800    597.592662     172800   \n",
      "58301     120000   172.161451     120000    172.161451     120000   \n",
      "58302     119200   702.330642     119200    702.330642     119200   \n",
      "58303     120000   523.702058     120000    523.702058     120000   \n",
      "58304     172800   407.062052     172800    407.062052     172800   \n",
      "58305     120000   137.276395     120000    137.276395     120000   \n",
      "58306     112565   647.429179     112565    647.429179     112565   \n",
      "58307     172800   321.547597     172800    321.547597     172800   \n",
      "58308      90000   150.243844      90000    150.243844      90000   \n",
      "58309     120000   163.615212     120000    163.615212     120000   \n",
      "58310     120000   728.079288     120000    728.079288     120000   \n",
      "\n",
      "           blur_sum  \n",
      "0       1445.218683  \n",
      "3       2435.694115  \n",
      "11       866.744832  \n",
      "14      2640.077889  \n",
      "25      5132.361409  \n",
      "28      3459.550233  \n",
      "31      1182.595210  \n",
      "35      6652.881103  \n",
      "46      5876.379771  \n",
      "50      1180.147631  \n",
      "55       538.319077  \n",
      "59      4384.845375  \n",
      "69      4012.378968  \n",
      "76       733.503790  \n",
      "78     75947.310848  \n",
      "98      3414.115125  \n",
      "103     6243.688150  \n",
      "113    10931.799540  \n",
      "118      174.825964  \n",
      "120     1142.537166  \n",
      "131      385.167895  \n",
      "134     1643.274310  \n",
      "138      268.413041  \n",
      "141      422.390108  \n",
      "146     2414.849393  \n",
      "148    12817.153645  \n",
      "154     4343.594865  \n",
      "157     8747.634142  \n",
      "165      826.017132  \n",
      "166     7629.910478  \n",
      "...             ...  \n",
      "58281    551.394874  \n",
      "58282   1041.103197  \n",
      "58283    653.349913  \n",
      "58284   1011.451891  \n",
      "58285   4649.885701  \n",
      "58286    768.365754  \n",
      "58287    610.308642  \n",
      "58288   2608.044153  \n",
      "58289    984.500109  \n",
      "58290    986.932739  \n",
      "58291    423.533488  \n",
      "58292   2327.543573  \n",
      "58293     83.859233  \n",
      "58294   3552.199833  \n",
      "58295   1631.107136  \n",
      "58296    216.246692  \n",
      "58297    198.197740  \n",
      "58298    385.470616  \n",
      "58299   2284.282272  \n",
      "58300    597.592662  \n",
      "58301    172.161451  \n",
      "58302    702.330642  \n",
      "58303    523.702058  \n",
      "58304    407.062052  \n",
      "58305    137.276395  \n",
      "58306    647.429179  \n",
      "58307    321.547597  \n",
      "58308    150.243844  \n",
      "58309    163.615212  \n",
      "58310    728.079288  \n",
      "\n",
      "[14652 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(image_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image as grayscale image\n",
    "from math import copysign, log10\n",
    "import numpy as np\n",
    "\n",
    "huMoments0=[]\n",
    "huMoments1=[]\n",
    "huMoments2=[]\n",
    "huMoments3=[]\n",
    "huMoments4=[]\n",
    "huMoments5=[]\n",
    "huMoments6=[]\n",
    "imageid =[]\n",
    "\n",
    "string=name.value + \"_images.zip\"\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", string)\n",
    "\n",
    "#Read the Zip File    \n",
    "with zipfile.ZipFile(filepath,\"r\") as zfile:\n",
    "      for filename in zfile.namelist():\n",
    "            data = zfile.read(filename)\n",
    "            \n",
    "            image = cv2.imdecode(np.frombuffer(data, np.uint8), 1)      \n",
    "            im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "            # Calculate Moments\n",
    "            moments = cv2.moments(im)\n",
    " \n",
    "            # Calculate Hu Moments\n",
    "            huMoments = cv2.HuMoments(moments)\n",
    "            # Log scale hu moments\n",
    "            for i in range(0,7):\n",
    "                  huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n",
    "                \n",
    "            #image id\n",
    "            imageid.append(filename.replace('.jpg',''))\n",
    "            huMoments0.append(huMoments[0])\n",
    "            \n",
    "            huMoments1.append(huMoments[1])\n",
    "            huMoments2.append(huMoments[2])\n",
    "            huMoments3.append(huMoments[3])\n",
    "            huMoments4.append(huMoments[4])\n",
    "            huMoments5.append(huMoments[5])\n",
    "            huMoments6.append(huMoments[6])\n",
    "\n",
    "image_moments = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n",
    "                           pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n",
    "                           pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n",
    "\n",
    "arr = np.array(data)\n",
    "\n",
    "# create the PetId variable\n",
    "image_moments['PetID'] = image_moments['ImageId'].str.split('-').str[0]\n",
    "image_moments = image_moments[image_moments['ImageId'].apply(lambda x:x.endswith((\"-1\")))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.494880Z",
     "start_time": "2019-03-05T22:04:35.824763Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "if name.value == \"train\":\n",
    "    df =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\train.csv')\n",
    "    \n",
    "    \n",
    "if name.value == \"test\":\n",
    "    df =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\test.csv')\n",
    "\n",
    "#test = pd.read_csv('https://raw.githubusercontent.com/alexlorenzo/Pet.my/master/test.csv')\n",
    "breed =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\n",
    "color =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\color_labels.csv') #A pet could have multiple colors\n",
    "state =pd.read_csv(r'C:\\Users\\alexandra\\Documents\\Kaggle\\state_labels.csv')\n",
    "\n",
    "# Add information about color, breed, state and sentiment data\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "df = (pd.merge(df, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n",
    "df = (pd.merge(df, state,  how='left', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n",
    "\n",
    "df = (pd.merge(df, sentimental_analysis,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "df = (pd.merge(df, sentimental_entities,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_metadata,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "# Add information about Metadata Images\n",
    "df = (pd.merge(df, image_properties,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "df = (pd.merge(df, image_moments,  how='left', left_on=['PetID'], right_on = ['PetID']))\n",
    "\n",
    "# Add information about quality Images\n",
    "df= (pd.merge(df, image_quality,  how='left', left_on=['PetID'], right_on = ['PetID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.559881Z",
     "start_time": "2019-03-05T22:04:42.496877Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malaysia Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:42.624875Z",
     "start_time": "2019-03-05T22:04:42.562882Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n",
    "\n",
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area ={\n",
    "    41336:19102,\n",
    "41325:9500,\n",
    "41367:15099,\n",
    "41401:243,\n",
    "41415:91,\n",
    "41324:1664,\n",
    "41332:6686,\n",
    "41335:36137,\n",
    "41330:21035,\n",
    "41380:821,\n",
    "41327:1048,\n",
    "41345:73631,\n",
    "41342:124450,\n",
    "41326:8104,\n",
    "41361:13035}\n",
    "\n",
    "state_unemployment ={\n",
    "    41336 : 3.6,\n",
    "41325 :2.9,\n",
    "41367: 3.8,\n",
    "41324: 0.9,\n",
    "41332 : 2.7,\n",
    "41335: 2.6,\n",
    "41330: 3.4,\n",
    "41380: 2.9,\n",
    "41327: 2.1,\n",
    "41345 : 5.4,\n",
    "41342 : 3.3,\n",
    "41326: 3.2,\n",
    "41361: 4.2,\n",
    "41415: 7.8,\n",
    "41401: 3.3\n",
    "}\n",
    "\n",
    "# per 1000 population\n",
    "state_birth_rate = {\n",
    " 41336:16.3,\n",
    "41325:17.0,\n",
    "41367:21.4,\n",
    "41401:14.4,\n",
    "41415:18.1,\n",
    "41324:16.0,\n",
    "41332:16.4,\n",
    "41335:17.0,\n",
    "41330:14.4,\n",
    "41380:17.5,\n",
    "41327:12.7,\n",
    "41345:13.7,\n",
    "41342:13.9,\n",
    "41326:16.6,\n",
    "41361:23.3,     \n",
    "}\n",
    "\n",
    "df[\"state_gdp\"] = df.State.map(state_gdp)\n",
    "df[\"state_population\"] = df.State.map(state_population)\n",
    "df[\"state_area\"] = df.State.map(state_area)\n",
    "df['state_unemployment']=df.State.map(state_unemployment)\n",
    "df['state_birth_rate']=df.State.map(state_birth_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:04:43.042880Z",
     "start_time": "2019-03-05T22:04:42.629882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n",
    "df['L_Color1'] = (pd.isnull(df['ColorName3']) & pd.isnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color2'] = (pd.isnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "df['L_Color3'] = (pd.notnull(df['ColorName3']) & pd.notnull(df['ColorName2']) & pd.notnull(df['ColorName1'])).astype(int)\n",
    "\n",
    "# Breed (create a flag if the pet has 1 breed or 2)\n",
    "df['L_Breed1'] = (pd.isnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "df['L_Breed2'] = (pd.notnull(df['BreedName2']) & pd.notnull(df['BreedName1'])).astype(int)\n",
    "\n",
    "# Breed create columns\n",
    "df['L_Breed1_Siamese'] =(df['BreedName1']=='Siamese').astype(int)\n",
    "df['L_Breed1_Persian']=(df['BreedName1']=='Persian').astype(int)\n",
    "df['L_Breed1_Labrador_Retriever']=(df['BreedName1']=='Labrador Retriever').astype(int)\n",
    "df['L_Breed1_Terrier']=(df['BreedName1']=='Terrier').astype(int)\n",
    "df['L_Breed1_Golden_Retriever ']=(df['BreedName1']=='Golden Retriever').astype(int)\n",
    "\n",
    "#Name (create a flag if the name is missing, with less than two letters)\n",
    "df['L_Name_missing'] =  (pd.isnull(df['Name'])).astype(int)\n",
    "df['Name_Length']=df['Name'].str.len() \n",
    "\n",
    "#Description \n",
    "df['Description_Length']=df['Description'].str.len() \n",
    "\n",
    "# Fee Amount\n",
    "df['L_Fee_Free'] =  (df['Fee']==0).astype(int)\n",
    "\n",
    "#Add the Number of Pets per Rescuer \n",
    "pets_total = df.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n",
    "df= pd.merge(df, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n",
    "df.count()\n",
    "\n",
    "# No photo\n",
    "df['L_NoPhoto'] =  (df['PhotoAmt']==0).astype(int)\n",
    "\n",
    "#No Video\n",
    "df['L_NoVideo'] =  (df['VideoAmt']==0).astype(int)\n",
    "\n",
    "#Log Age \n",
    "df['Log_Age']= np.log(df.Age+1) \n",
    "\n",
    "#Negative Score \n",
    "df['L_scoreneg'] =  (df['sentiment_document_score']<0).astype(int)\n",
    "\n",
    "#Quantity Amount >5\n",
    "df.loc[df['Quantity'] > 5, 'Quantity'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:04.540006Z",
     "start_time": "2019-03-05T22:04:43.045884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the Variable Description\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cleaning_text(df,column):  \n",
    "    df[column] =df[column].fillna(\"<MISSING>\") #Replace missing values\n",
    "    df[column] = df[column].str.replace('\\d+', '') #Remove numbers\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace('[^\\w\\s]','') #Remove punctuactions\n",
    "    df[column] = df[column].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ') #Delete duplicated word\n",
    "\n",
    "    stop = stopwords.words('english') #Do not take into account stop words (ex: The)\n",
    "\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df[column] = df[column].str.replace(pat, '')\n",
    "    df[column] = df[column].str.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Stem Words\n",
    "    df[column] = df[column].astype(str).str.split()\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    df[column]=df[column].apply(lambda x : [porter_stemmer.stem(y) for y in x]) #Word of the same family\n",
    "\n",
    "    df[column]=df[column].apply(lambda x : \" \".join(x))\n",
    "    return df\n",
    "\n",
    "df = cleaning_text(df,'Description')\n",
    "df = cleaning_text(df,'sentiment_entities_name')\n",
    "df = cleaning_text (df,'metadata_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.551811Z",
     "start_time": "2019-03-05T22:05:04.541990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix Factorization for dimensionality reduction\n",
    "def matrix_factorization(df,column,prefix_svd, prefix_nmf):\n",
    "    \n",
    "    svd_ = TruncatedSVD(\n",
    "        n_components=5, random_state=1337)\n",
    "    nmf_ = NMF(\n",
    "        n_components=5, random_state=1337)\n",
    "\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(df[column])\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix(prefix_svd)\n",
    "\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix(prefix_nmf)\n",
    "    df = pd.concat([df,nmf_col,svd_col],axis=1)\n",
    "    return df\n",
    "    \n",
    "df = matrix_factorization(df,'Description','SVD_Desc_','NMF_Desc_')\n",
    "df = matrix_factorization(df,'sentiment_entities_name','SVD_entities_','NMF_entities_')\n",
    "df = matrix_factorization(df,'metadata_description','SVD_metadata_desc_','NMF_metadata_desc_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.587813Z",
     "start_time": "2019-03-05T22:05:06.553815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cannot be used for this analysis (IDs, Texts...)\n",
    "#df = df.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID',#'Description',\n",
    "#              'BreedName1','Color1', 'Color2', 'Color3','Age','State','metadata_description','sentiment_entities_name'\n",
    "#              'ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength','StateName'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "\n",
    "missing_value_df =missing_value_df[missing_value_df['percent_missing']>0]\n",
    "missing_value_df\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x=\"column_name\", y=\"percent_missing\", data=missing_value_df, label='Sales')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=75)\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % round(p.get_height(),2) + '%', \n",
    "            fontsize=12, color='grey', ha='center', va='bottom') \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:06.627814Z",
     "start_time": "2019-03-05T22:05:06.588811Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n",
    "           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n",
    "           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max','pixel_sum',\n",
    "           'blur_min','blur_max','blur_sum','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n",
    "           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n",
    "            'Name_Length','Description_Length','huMoments0','huMoments1','huMoments2','huMoments3','huMoments4','huMoments5','huMoments6']:\n",
    "    \n",
    "\n",
    "    df[col].fillna((df[col].median()), inplace=True)\n",
    "    \n",
    "# replacing na values with No Color \n",
    "df[\"ColorName2\"].fillna(\"No Color\", inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T22:05:07.026816Z",
     "start_time": "2019-03-05T22:05:06.630816Z"
    }
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "#df = pd.concat([df.drop('StateName', axis=1),pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['StateName'], prefix='State')], axis=1)\n",
    "\n",
    "col=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\n",
    "for i in col:\n",
    "    #df = pd.concat([df.drop(i, axis=1),pd.get_dummies(df[i], prefix=i)], axis=1)\n",
    "    df = pd.concat([df,pd.get_dummies(df[i], prefix=i)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= name.value + \"_features.csv\"\n",
    "print(output)\n",
    "filepath= os.path.join(r\"C:\\Users\\alexandra\\Documents\\Kaggle\", output)\n",
    "df.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "712px",
    "left": "1622px",
    "top": "111.133px",
    "width": "200px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.233334,
   "position": {
    "height": "553.233px",
    "left": "1533px",
    "right": "37px",
    "top": "163px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
